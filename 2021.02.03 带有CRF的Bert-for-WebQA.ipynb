{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-for-WebQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " sformers 包从零开始搭建了一个中文阅读问答训练测试\n",
    "框架 ，数据选择百度的 WebQA 问答数据集， 类似于斯坦福智能问答数据集，\n",
    "使用 Bert-base-chinese 和 CRF 模型 做基础，模型可以根据需要持续更新。\n",
    "\n",
    "## 模型\n",
    "\n",
    "输入：[‘CLS’]+Question+[‘SEP’]+Evidence 字符串\n",
    "\n",
    "模型框架：采用多任务联合训练的方式，共两个任务：\n",
    "\n",
    "           任务1. 使用\"[CLS]\"来判断两个句子是否是Quesntion-Evidence的关系；\n",
    "\n",
    "           任务2. 使用Question+[‘SEP’]+Evidence的BERT表达 + CRF模型 进行序列标注，找出Evidence中的答案。\n",
    "\n",
    "输出：\n",
    "\n",
    "           任务1. [batch_size,1] 的0-1 序列，1表示对应的文章中含有问题答案，0表示没有；\n",
    "           \n",
    "           任务2. [batch_size, seq_len] 的0-1 序列, Evidence 中出现答案的位置为 1 ，其余为 0。\n",
    "\n",
    "备注： 选择使用\"[CLS]\"做Quesntion-Evidence关系判断的原因是，做大规模文档检索时，通常回返回一些带有迷惑性的负样本，用\"[CLS]\"可以进行二次过滤。\n",
    "\n",
    "#### 训练精度     \n",
    "\n",
    "           Eval On TestData   Eval-Loss: 15.383  Eval-Result（召回）: R = 0.796\n",
    "\n",
    "           Eval On DevData    Eval-Loss: 13.986  Eval-Result（召回）: R = 0.795\n",
    "\n",
    "数据集来自：https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A 提取码：2dva\n",
    "\n",
    "BaseLine论文：https://arxiv.org/abs/1607.06275\n",
    "\n",
    "模型的谷歌云共享连接(训练好的模型)：https://drive.google.com/open?id=1KHlCnT6VEpDCvtJp8FfwMtU5_ABrYzH9\n",
    "\n",
    "==================== 超参 ====================\n",
    "\n",
    "           early_stop = 1\n",
    "                   lr = 1e-05\n",
    "                   l2 = 1e-05\n",
    "             n_epochs = 5\n",
    "            Negweight = 0.01\n",
    "             trainset = data/me_train.json\n",
    "               devset = data/me_validation.ann.json\n",
    "              testset = data/me_test.ann.json\n",
    "       knowledge_path = data/me_test.ann.json\n",
    "        Stopword_path = data/stop_words.txt\n",
    "               device = cuda\n",
    "                 mode = train\n",
    "           model_path = save_model/latest_model.pt\n",
    "           model_back = save_model/back_model.pt\n",
    "           batch_size = 16\n",
    "\n",
    "\n",
    "说明：上面效果只训练了半个epoch 因为疫情在家没有服务器，用谷歌云训练的，设备是tesla-P100，回答一个问题平均耗时40ms。\n",
    "\n",
    "## 问答模块\n",
    "\n",
    "问答模块设计了两种功能：\n",
    "\n",
    "1.带有文章的阅读问答；\n",
    "\n",
    "2.根据问题从知识库中快速检索文章，再进行阅读问答的智能问答，问题的答案要在知识库里面有才行！\n",
    " \n",
    "\n",
    "## 文档检索\n",
    "\n",
    "           步骤-0 准备知识库 \n",
    "\n",
    "           步骤-1 jieba分词 \n",
    "\n",
    "           步骤-2 去停用词 \n",
    "\n",
    "           步骤-3 基于分词和二元语法词袋，使用sklearn计算TF-IDF矩阵 \n",
    "\n",
    "           步骤-4 根据Query和知识库的TF-IDF矩阵计算排序出相关度较高的10篇文章。\n",
    "           \n",
    "用测试集数据搭建的知识库，文章检索精度 89%，其中文章数为3024，根据一个Query一次筛选出15篇文章，有89%的概率包含正确Evidene。\n",
    "\n",
    "\n",
    "## 运行\n",
    "\n",
    "           训练 %run TrainAndEval.py --batch_size=8 --mode=\"train\" --model_path='save_model/latest_model.pt'\n",
    "\n",
    "           评估 %run TrainAndEval.py --mode=\"eval\" --model_path='save_model/latest_model.pt'\n",
    "\n",
    "           阅读问答 %run TrainAndEval.py  --mode=\"demo\" --model_path='save_model/latest_model.pt'\n",
    "\n",
    "           智能问答 %run TrainAndEval.py  --mode=\"QA\" --model_path='save_model/latest_model.pt'\n",
    "\n",
    "## 不足\n",
    "\n",
    "1. 目前模型对正确的Evidence能高准确度识别出正确答案，但是很难分辨有迷惑性的错误Evidence，下一步需要对\"[CLS]\"识别错误Evidence进行提升。这会导致在智能问答模块，识别出多个包含正确答案的候选答案，却无法确定哪一个是唯一正确答案。\n",
    "\n",
    "2. 大规模文档检索时，因词袋较大，TF-IDF矩阵计算会很慢，下一步会根据FaceBook/DrQA文档检索模块，使用稀疏矩阵和哈希特征进行改进。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    FILE :  CRF.py\n",
    "    FUNCTION : None\n",
    "    REFERENCE : https://github.com/jiesutd/NCRFpp/blob/master/model/crf.py\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def log_sum_exp(vec, m_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vec: size=(batch_size, vanishing_dim, hidden_dim)\n",
    "        m_size: hidden_dim\n",
    "    Returns:\n",
    "        size=(batch_size, hidden_dim)\n",
    "    \"\"\"\n",
    "    _, idx = torch.max(vec, 1)  # B * 1 * M\n",
    "    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n",
    "    return max_score.view(-1, m_size) + torch.log(torch.sum(\n",
    "        torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \"\"\"\n",
    "        CRF\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        kwargs:\n",
    "            target_size: int, target size\n",
    "            device: str, device\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__()\n",
    "        for k in kwargs:\n",
    "            self.__setattr__(k, kwargs[k])\n",
    "        device = self.device\n",
    "\n",
    "        # init transitions\n",
    "        self.START_TAG, self.STOP_TAG = -2, -1\n",
    "        init_transitions = torch.zeros(self.target_size + 2, self.target_size + 2, device=device)\n",
    "        init_transitions[:, self.START_TAG] = -10000.0\n",
    "        init_transitions[self.STOP_TAG, :] = -10000.0\n",
    "        self.transitions = nn.Parameter(init_transitions)\n",
    "\n",
    "    def _forward_alg(self, feats, mask):\n",
    "        \"\"\"\n",
    "        Do the forward algorithm to compute the partition function (batched).\n",
    "        Args:\n",
    "            feats: size=(batch_size, seq_len, self.target_size+2)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "        Returns:\n",
    "            xxx\n",
    "        \"\"\"\n",
    "        #print(feats.shape)\n",
    "        batch_size = feats.size(0)\n",
    "        seq_len = feats.size(1)\n",
    "        tag_size = feats.size(2)\n",
    "        mask = mask.transpose(1, 0).contiguous().bool()\n",
    "        ins_num = seq_len * batch_size\n",
    "        \"\"\" be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1) \"\"\"\n",
    "        feats = feats.transpose(1,0).contiguous().view(ins_num,1, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "        \"\"\" need to consider start \"\"\"\n",
    "        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
    "        # build iter\n",
    "        seq_iter = enumerate(scores)\n",
    "        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n",
    "        \"\"\" only need start from start_tag \"\"\"\n",
    "        partition = inivalues[:, self.START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n",
    "\n",
    "        \"\"\"\n",
    "        add start score (from start to all tag, duplicate to batch_size)\n",
    "        partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n",
    "        iter over last scores\n",
    "        \"\"\"\n",
    "        for idx, cur_values in seq_iter:\n",
    "            \"\"\"\n",
    "            previous to_target is current from_target\n",
    "            partition: previous results log(exp(from_target)), #(batch_size * from_target)\n",
    "            cur_values: bat_size * from_target * to_target\n",
    "            \"\"\"\n",
    "            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
    "            cur_partition = log_sum_exp(cur_values, tag_size)\n",
    "\n",
    "            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n",
    "\n",
    "            \"\"\" effective updated partition part, only keep the partition value of mask value = 1 \"\"\"\n",
    "            masked_cur_partition = cur_partition.masked_select(mask_idx)\n",
    "            \"\"\" let mask_idx broadcastable, to disable warning \"\"\"\n",
    "            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n",
    "\n",
    "            \"\"\" replace the partition where the maskvalue=1, other partition value keeps the same \"\"\"\n",
    "            partition.masked_scatter_(mask_idx, masked_cur_partition)\n",
    "        \"\"\" \n",
    "        until the last state, add transition score for all partition (and do log_sum_exp) \n",
    "        then select the value in STOP_TAG \n",
    "        \"\"\"\n",
    "        cur_values = self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
    "        cur_partition = log_sum_exp(cur_values, tag_size)\n",
    "        final_partition = cur_partition[:, self.STOP_TAG]\n",
    "        return final_partition.sum(), scores\n",
    "\n",
    "    def _viterbi_decode(self, feats, mask):\n",
    "        \"\"\"\n",
    "            input:\n",
    "                feats: (batch, seq_len, self.tag_size+2)\n",
    "                mask: (batch, seq_len)\n",
    "            output:\n",
    "                decode_idx: (batch, seq_len) decoded sequence\n",
    "                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n",
    "        \"\"\"\n",
    "        # print(feats.size())\n",
    "        batch_size = feats.size(0)\n",
    "        seq_len = feats.size(1)\n",
    "        tag_size = feats.size(2)\n",
    "        # assert(tag_size == self.tagset_size+2)\n",
    "        \"\"\" calculate sentence length for each sentence \"\"\"\n",
    "        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n",
    "        \"\"\" mask to (seq_len, batch_size) \"\"\"\n",
    "        mask = mask.transpose(1, 0).contiguous().bool()\n",
    "        ins_num = seq_len * batch_size\n",
    "        \"\"\" be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1) \"\"\"\n",
    "        feats = feats.transpose(1,0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "        \"\"\" need to consider start \"\"\"\n",
    "        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
    "\n",
    "        # build iter\n",
    "        seq_iter = enumerate(scores)\n",
    "        # record the position of best score\n",
    "        back_points = list()\n",
    "        partition_history = list()\n",
    "        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n",
    "        # mask = 1 + (-1)*mask\n",
    "        mask = (1 - mask.long()).byte().bool()\n",
    "        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n",
    "        \"\"\" only need start from start_tag \"\"\"\n",
    "        partition = inivalues[:, self.START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n",
    "        partition_history.append(partition)\n",
    "        # iter over last scores\n",
    "        for idx, cur_values in seq_iter:\n",
    "            \"\"\"\n",
    "            previous to_target is current from_target\n",
    "            partition: previous results log(exp(from_target)), #(batch_size * from_target)\n",
    "            cur_values: batch_size * from_target * to_target\n",
    "            \"\"\"\n",
    "            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
    "            \"\"\" forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG \"\"\"\n",
    "            partition, cur_bp = torch.max(cur_values, 1)\n",
    "            partition_history.append(partition)\n",
    "            \"\"\"\n",
    "            cur_bp: (batch_size, tag_size) max source score position in current tag\n",
    "            set padded label as 0, which will be filtered in post processing\n",
    "            \"\"\"\n",
    "            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n",
    "            back_points.append(cur_bp)\n",
    "        \"\"\" add score to final STOP_TAG \"\"\"\n",
    "        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1, 0).contiguous() ## (batch_size, seq_len. tag_size)\n",
    "        \"\"\" get the last position for each setences, and select the last partitions using gather() \"\"\"\n",
    "        last_position = length_mask.view(batch_size,1,1).expand(batch_size, 1, tag_size) -1\n",
    "        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)\n",
    "        \"\"\" calculate the score from last partition to end state (and then select the STOP_TAG from it) \"\"\"\n",
    "        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1,tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n",
    "        _, last_bp = torch.max(last_values, 1)\n",
    "        pad_zero = torch.zeros(batch_size, tag_size, device=self.device, requires_grad=True).long()\n",
    "        back_points.append(pad_zero)\n",
    "        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n",
    "\n",
    "        \"\"\" elect end ids in STOP_TAG \"\"\"\n",
    "        pointer = last_bp[:, self.STOP_TAG]\n",
    "        insert_last = pointer.contiguous().view(batch_size,1,1).expand(batch_size,1, tag_size)\n",
    "        back_points = back_points.transpose(1,0).contiguous()\n",
    "        \"\"\"move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values \"\"\"\n",
    "        back_points.scatter_(1, last_position, insert_last)\n",
    "        back_points = back_points.transpose(1,0).contiguous()\n",
    "        \"\"\" decode from the end, padded position ids are 0, which will be filtered if following evaluation \"\"\"\n",
    "        # decode_idx = Variable(torch.LongTensor(seq_len, batch_size))\n",
    "        decode_idx = torch.empty(seq_len, batch_size, device=self.device, requires_grad=True).long()\n",
    "        decode_idx[-1] = pointer.detach()\n",
    "        for idx in range(len(back_points)-2, -1, -1):\n",
    "            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n",
    "            decode_idx[idx] = pointer.detach().view(batch_size)\n",
    "        path_score = None\n",
    "        decode_idx = decode_idx.transpose(1, 0)\n",
    "        return path_score, decode_idx\n",
    "\n",
    "    def forward(self, feats, mask):\n",
    "        \"\"\"\n",
    "        :param feats:\n",
    "        :param mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        path_score, best_path = self._viterbi_decode(feats, mask)\n",
    "        return path_score, best_path\n",
    "\n",
    "    def _score_sentence(self, scores, mask, tags):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scores: size=(seq_len, batch_size, tag_size, tag_size)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "            tags: size=(batch_size, seq_len)\n",
    "        Returns:\n",
    "            score:\n",
    "        \"\"\"\n",
    "        # print(scores.size())\n",
    "        batch_size = scores.size(1)\n",
    "        seq_len = scores.size(0)\n",
    "        tag_size = scores.size(-1)\n",
    "        tags = tags.view(batch_size, seq_len)\n",
    "        \"\"\" convert tag value into a new format, recorded label bigram information to index \"\"\"\n",
    "        # new_tags = Variable(torch.LongTensor(batch_size, seq_len))\n",
    "        new_tags = torch.empty(batch_size, seq_len, device=self.device, requires_grad=True).long()\n",
    "        for idx in range(seq_len):\n",
    "            if idx == 0:\n",
    "                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n",
    "            else:\n",
    "                new_tags[:, idx] = tags[:, idx-1] * tag_size + tags[:, idx]\n",
    "\n",
    "        \"\"\" transition for label to STOP_TAG \"\"\"\n",
    "        end_transition = self.transitions[:, self.STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n",
    "        \"\"\" length for batch,  last word position = length - 1 \"\"\"\n",
    "        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n",
    "        \"\"\" index the label id of last word \"\"\"\n",
    "        end_ids = torch.gather(tags, 1, length_mask-1)\n",
    "\n",
    "        \"\"\" index the transition score for end_id to STOP_TAG \"\"\"\n",
    "        end_energy = torch.gather(end_transition, 1, end_ids)\n",
    "\n",
    "        \"\"\" convert tag as (seq_len, batch_size, 1) \"\"\"\n",
    "        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n",
    "        \"\"\" need convert tags id to search from 400 positions of scores \"\"\"\n",
    "        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len, batch_size)\n",
    "        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n",
    "\n",
    "        \"\"\"\n",
    "        add all score together\n",
    "        gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n",
    "        \"\"\"\n",
    "        gold_score = tg_energy.sum() + end_energy.sum()\n",
    "\n",
    "        return gold_score\n",
    "\n",
    "    def neg_log_likelihood_loss(self, feats, mask, tags):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feats: size=(batch_size, seq_len, tag_size)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "            tags: size=(batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = feats.size(0)\n",
    "        forward_score, scores = self._forward_alg(feats, mask)\n",
    "        gold_score = self._score_sentence(scores, mask, tags)\n",
    "        return forward_score - gold_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-for-WebQA   \n",
    "使用 torch 和 transformers 包从零开始搭建了一个中文阅读问答训练测试 框架 ，数据选择百度的 WebQA 问答数据集， 类似于斯坦福智能问答数据集， 使用 Bert-base-chinese 和 CRF 模型 做基础，模型可以根据需要持续更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils import data\n",
    "from transformers import BertTokenizer , BertModel\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "#from DocumentRetrieval import Knowledge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import random\n",
    "import jieba\n",
    "import collections\n",
    "# 字符ID化\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '毛主席出生在哪一年？'\n",
    "e = '毛主席于1896年出生在湖南长沙韶山冲'\n",
    "\n",
    "tokens = tokenizer.tokenize('[CLS]'+q+'[SEP]'+e)# list\n",
    "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_type_ids = [0 if i <= tokens_id.index(102) else 1 for i in range(len(tokens_id ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    tokens_l, tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l= list(map(list, zip(*batch)))\n",
    "    maxlen = np.array([len(sen) for sen in tokens_l]).max()\n",
    "    ### pad和截断\n",
    "    for i in range(len(tokens_l)):\n",
    "        tokens = tokens_l[i]\n",
    "        tokens_id= tokens_id_l[i]\n",
    "        # answer_offset = answer_offset_l[i]\n",
    "        answer_seq_label = answer_seq_label_l[i]\n",
    "        token_type_ids = token_type_ids_l[i]\n",
    "        tokens_l[i] = tokens + (maxlen - len(tokens))*['[PAD]']\n",
    "        token_type_ids_l[i] = token_type_ids + (maxlen - len(tokens))*[1]\n",
    "        tokens_id_l[i] =tokens_id + (maxlen - len(tokens))*tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
    "        answer_seq_label_l[i] = answer_seq_label + [0]*(maxlen - len(tokens))\n",
    "    return tokens_l, tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l\n",
    "\n",
    "def result_metric(prediction_all, y_2d_all):\n",
    "    total_num=0\n",
    "    toral_cur=0\n",
    "    for prediction, y_2d in zip(prediction_all, y_2d_all):\n",
    "        batch_size,seq_len = prediction.size()\n",
    "        currect = torch.sum(torch.sum(prediction == y_2d, dim=1)==seq_len).to(\"cpu\").item()\n",
    "        toral_cur = toral_cur + currect\n",
    "        total_num = total_num + batch_size\n",
    "    return toral_cur/total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebQADataset(data.Dataset):\n",
    "    def __init__(self, fpath):\n",
    "        self.hp = hp\n",
    "        self.questions, self.evidences, self.answer= [], [], []\n",
    "        with open(fpath, 'r',encoding='utf-8') as f:\n",
    "            data = json.load(f)#读取json文件内容\n",
    "            for key in data:\n",
    "                item = data[key]\n",
    "                question = item['question']\n",
    "                evidences = item['evidences']\n",
    "                for evi_key in evidences:\n",
    "                    evi_item = evidences[evi_key]\n",
    "                    self.questions.append(question)\n",
    "                    self.evidences.append(evi_item['evidence'])\n",
    "                    self.answer.append(evi_item['answer'][0])\n",
    "        shuffled_l = list(zip(self.questions, self.evidences, self.answer))\n",
    "        random.shuffle(shuffled_l)\n",
    "        self.questions[:], self.evidences[:], self.answer[:] = zip(*shuffled_l)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answer)\n",
    "\n",
    "    def FindOffset(self, tokens_id, answer_id):\n",
    "        n = len(tokens_id)\n",
    "        m = len(answer_id)\n",
    "        if n < m:\n",
    "            return False\n",
    "        for i in range(n - m + 1):\n",
    "            if tokens_id[i:i + m] == answer_id:\n",
    "                return (i, i + m)\n",
    "        return False\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # We give credits only to the first piece.\n",
    "        q, e, a = self.questions[idx], self.evidences[idx], self.answer[idx]\n",
    "        tokens = tokenizer.tokenize('[CLS]'+q+'[SEP]'+e)# list\n",
    "        if len(tokens)>256:\n",
    "            tokens=tokens[:256]\n",
    "        tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_type_ids = [0 if i <= tokens_id.index(102) else 1 for i in range(len(tokens_id ))]\n",
    "        answer_offset = (-1, -1)\n",
    "        IsQA = 0\n",
    "        answer_seq_label = len(tokens_id) * [0]\n",
    "        if a != 'no_answer':\n",
    "            answer_tokens = tokenizer.tokenize(a)\n",
    "            answer_offset = self.FindOffset(tokens, answer_tokens)#有肯能返回False\n",
    "            if answer_offset:#在原文中找到答案\n",
    "                answer_seq_label[answer_offset[0]:answer_offset[1]] = [1]*(len(answer_tokens))\n",
    "                IsQA = 1\n",
    "            else:# self.FindOffset 返回False\n",
    "                answer_offset = (-1, -1)\n",
    "        return tokens, tokens_id, token_type_ids, answer_offset, answer_seq_label, IsQA\n",
    "    \n",
    "    def get_samples_weight(self,Negweight):\n",
    "        samples_weight = []\n",
    "        for ans in self.answer:\n",
    "            if ans != 'no_answer':\n",
    "                samples_weight.append(1.0)\n",
    "            else:\n",
    "                samples_weight.append(Negweight)\n",
    "        return np.array(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepare_Train_Features_For_CRF:\n",
    "    def __init__(self,tokenizer,max_length = 384,stride = 128,pad_on_right = \"right\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.pad_on_right = pad_on_right\n",
    "\n",
    "    def prepare_train_features(self,examples):\n",
    "        # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = self.tokenizer(\n",
    "            examples[\"question\" if self.pad_on_right else \"context\"],\n",
    "            examples[\"context\" if self.pad_on_right else \"question\"],\n",
    "            truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
    "            max_length= self.max_length,\n",
    "            stride= self.stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "        tokenized_examples[\"answer_offset\"] = []\n",
    "        tokenized_examples[\"answer_seq_label\"] = []\n",
    "        tokenized_examples[\"labels\"] = []\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            answer_seq_label = len(input_ids) * [0]\n",
    "            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
    "\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[\"answers\"]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"answer_offset\"].append((-1,-1))\n",
    "                tokenized_examples[\"labels\"].append(0)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"answer_offset\"].append((-1,-1))\n",
    "                    tokenized_examples[\"labels\"].append(0)\n",
    "\n",
    "                else:\n",
    "                    tokenized_examples[\"labels\"].append(1)\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "                    tokenized_examples[\"answer_offset\"].append((token_start_index - 1,token_end_index + 1))\n",
    "                    answer_tokens = self.tokenizer.tokenize(answers[\"text\"][0])\n",
    "                    answer_seq_label[token_start_index - 1:token_end_index + 1] = [1]*(len(answer_tokens))\n",
    "                    print(len(answer_seq_label))\n",
    "                    tokenized_examples[\"answer_seq_label\"].append(answer_seq_label)\n",
    "               \n",
    "\n",
    "        return tokenized_examples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = r'C:\\Users\\Administrator\\Desktop\\2021.02.08 multi_choice型阅读理解\\bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_features = Prepare_Train_Features_For_CRF(tokenizer,max_length = 384,stride = 128,pad_on_right = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 4080, 1220, 4638, 1333, 1728, 3221, 784, 720, 8043, 102, 2496, 2769, 4692, 1168, 2456, 6379, 6158, 7023, 5287, 8024, 6956, 1999, 7566, 2193, 1091, 5314, 2769, 4638, 1726, 928, 3198, 8024, 2769, 4761, 6887, 2769, 3633, 1762, 711, 6821, 702, 1744, 2157, 4638, 1355, 2245, 2226, 4708, 671, 819, 1213, 7030, 8024, 8149, 3189, 8024, 3777, 1266, 4689, 6928, 1378, 7167, 7188, 3300, 7361, 1062, 1385, 4638, 3249, 6858, 2339, 782, 4635, 7032, 6645, 8024, 2897, 4708, 1325, 2399, 3341, 1744, 2157, 1392, 6956, 1999, 1353, 7668, 5314, 800, 4638, 2697, 6468, 928, 8024, 4080, 1220, 1765, 2190, 704, 3173, 5381, 6381, 5442, 6432, 8024, 8149, 2399, 3341, 8024, 1744, 2157, 1062, 2128, 6956, 1744, 2157, 2339, 1555, 2600, 2229, 1744, 2157, 4906, 2110, 2825, 3318, 1999, 1447, 833, 4906, 2825, 6956, 1310, 4495, 6956, 1744, 2157, 1355, 2245, 3121, 7484, 1999, 1447, 833, 5023, 6956, 1999, 1772, 2970, 1358, 2400, 7023, 5287, 6814, 4638, 2769, 4638, 2456, 6379, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'start_positions': [111], 'end_positions': [160], 'answer_offset': [(111, 160)], 'answer_seq_label': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = {'answers': {'answer_start': [102],\n",
    "  'text': ['国家公安部国家工商总局国家科学技术委员会科技部卫生部国家发展改革委员会等部委均接受并采纳过的我的建议']},\n",
    " 'context': '当我看到建议被采纳，部委领导写给我的回信时，我知道我正在为这个国家的发展尽着一份力量，27日，河北省邢台钢铁有限公司的普通工人白金跃，拿着历年来国家各部委反馈给他的感谢信，激动地对中新网记者说，27年来，国家公安部国家工商总局国家科学技术委员会科技部卫生部国家发展改革委员会等部委均接受并采纳过的我的建议',\n",
    " 'id': 's000011',\n",
    " 'question': '激动的原因是什么？',\n",
    " 'title': ''}\n",
    "\n",
    "prepare_features.prepare_train_features(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'WebQA.v1.0'\n",
    "trainset_path = 'me_train.json'\n",
    "dev_path = 'me_validation.ann.json'\n",
    "test_path = 'me_test.ann.json'\n",
    "mode_path = r'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WebQADataset(os.path.join(dir_path ,trainset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7b3c8812e66c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 返回结果：tokens, tokens_id, token_type_ids, answer_offset, answer_seq_label, IsQA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 返回结果：tokens, tokens_id, token_type_ids, answer_offset, answer_seq_label, IsQA\n",
    "print(train_dataset[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-80b189294c0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_dataset[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BertModel.from_pretrained(r'bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.tensor([1,55,55,55,66,4]).view(1,-1)\n",
    "#token_type_ids = torch.tensor([0,0,0,0,0,0]).view(1,-1)\n",
    "\n",
    "#outputs = model(input_ids,token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb, _ = self.PreModel(input_ids=tokens_x_2d, token_type_ids=token_type_ids_2d) #[batch_size, seq_len, hidden_size]\n",
    "#last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) \n",
    "#pooler_output (torch.FloatTensor of shape (batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForQuestionAnswering, BertPreTrainedModel,BertModel\n",
    "import numpy as np\n",
    "\n",
    "class BertForQuestionAnsweringWithCRF(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.CRF_fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.hidden_size, config.num_labels + 2, bias=True),\n",
    "        )\n",
    "        self.CRF = CRF(target_size = self.bert.config.num_labels,device= torch.device(\"cuda\"))\n",
    "        self.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 2, bias=True)\n",
    "\n",
    "    def forward(self,tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l):\n",
    "\n",
    "\n",
    "        ## 字符ID [batch_size, seq_length]\n",
    "        tokens_x_2d = torch.LongTensor(tokens_id_l).to(self.device)\n",
    "        token_type_ids_2d = torch.LongTensor(token_type_ids_l).to(self.device)\n",
    "\n",
    "        # 计算sql_len 不包含[CLS]\n",
    "        batch_size, seq_length = tokens_x_2d[:,1:].size()\n",
    "\n",
    "        ## CRF答案ID [batch_size, seq_length]\n",
    "        y_2d = torch.LongTensor(answer_seq_label_l).to(self.device)[:,1:]\n",
    "        ## (batch_size,)\n",
    "        y_IsQA_2d = torch.LongTensor(IsQA_l).to(self.device)\n",
    "\n",
    "\n",
    "        if self.training:    # self.training基层的外部类\n",
    "            self.bert.train()\n",
    "            output = self.bert(input_ids=tokens_x_2d, token_type_ids=token_type_ids_2d, output_hidden_states= True,return_dict= True)  #[batch_size, seq_len, hidden_size]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.bert(input_ids=tokens_x_2d, token_type_ids=token_type_ids_2d, output_hidden_states= True,return_dict= True)\n",
    "\n",
    "        ## [CLS] for IsQA  [batch_size, hidden_size]\n",
    "        cls_emb = output.last_hidden_state[:,0,:] \n",
    "        \n",
    "        IsQA_logits = self.fc2(cls_emb) ## [batch_size, 2]\n",
    "        IsQA_loss = self.CrossEntropyLoss.forward(IsQA_logits,y_IsQA_2d)\n",
    "\n",
    "        ## [batch_size, 1]\n",
    "        IsQA_prediction = IsQA_logits.argmax(dim=-1).unsqueeze(dim=-1)\n",
    "\n",
    "        # CRF mask\n",
    "        mask = np.ones(shape=[batch_size, seq_length], dtype=np.uint8)\n",
    "        mask = torch.ByteTensor(mask).bool().to(self.device)  # [batch_size, seq_len, 4]\n",
    "      \n",
    "\n",
    "        # No [CLS]\n",
    "        crf_logits = self.CRF_fc1(output.last_hidden_state[:,1:,:] )\n",
    "        crf_loss = self.CRF.neg_log_likelihood_loss(feats=crf_logits, mask=mask, tags=y_2d )\n",
    "        _, CRFprediction = self.CRF.forward(feats=crf_logits, mask=mask)\n",
    "\n",
    "        return IsQA_prediction, CRFprediction, IsQA_loss, crf_loss, y_2d, y_IsQA_2d.unsqueeze(dim=-1)# (batch_size,) -> (batch_size, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--early_stop\", type=int, default=1)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--l2\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=5)\n",
    "parser.add_argument(\"--Negweight\", type=float, default=0.01)\n",
    "\n",
    "parser.add_argument(\"--knowledge_path\", type=str, default=r\"WebQA.v1.0\\me_test.ann.json\")\n",
    "parser.add_argument(\"--Stopword_path\",type=str, default= 'data/stop_words.txt')\n",
    "parser.add_argument(\"--device\", type=str, default='cuda')\n",
    "parser.add_argument(\"--mode\", type=str, default='train')  # eval / demo / train / QA\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"save_model/latest_model.bin\")\n",
    "parser.add_argument(\"--model_back\", type=str, default=\"save_model/back_model.bin\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=12)\n",
    "\n",
    "hp = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainOneEpoch(model, train_iter, dev_iter, test_iter, optimizer, hp):\n",
    "    model.train()\n",
    "    CRFprediction_all, CRFloss_all, IsQAloss_all, y_CRF_all, IsQA_prediction_all, y_IsQA_all= [],[],[],[],[],[]\n",
    "\n",
    "    best_acc = 0\n",
    "    for i, batch in enumerate(tqdm(train_iter)):\n",
    "        _, tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l = batch\n",
    "        optimizer.zero_grad()\n",
    "        IsQA_prediction, CRFprediction, IsQA_loss, crf_loss, y_2d, y_IsQA_2d  = model.forward(tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l)\n",
    "        ## CRF\n",
    "        CRFprediction_all.append(CRFprediction)\n",
    "        y_CRF_all.append(y_2d)\n",
    "\n",
    "        ## IsQA\n",
    "        IsQA_prediction_all.append(IsQA_prediction)\n",
    "\n",
    "        y_IsQA_all.append(y_IsQA_2d)\n",
    "\n",
    "        # loss\n",
    "        CRFloss_all.append(crf_loss.to(\"cpu\").item())\n",
    "        IsQAloss_all.append(IsQA_loss.to(\"cpu\").item())\n",
    "        loss = 0.2*IsQA_loss + 0.8*crf_loss\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 3.0)#设置梯度截断阈值\n",
    "        loss.backward()## 计算梯度\n",
    "        optimizer.step()## 根据计算的梯度更新网络参数\n",
    "\n",
    "\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            accCRF = result_metric(CRFprediction_all, y_CRF_all)\n",
    "            accIsQA = result_metric(IsQA_prediction_all, y_IsQA_all)\n",
    "\n",
    "            print(\"<Last 100 Steps MeanValue> Setp-{} IsQA-Loss: {:.3f} CRF-Loss: {:.3f}  \"\n",
    "                  \"CRF-Result: accCRF = {:.3f}  IsQA-Result: accIsQA = {:.3f}\"\n",
    "                  .format(i,np.mean(IsQAloss_all), np.mean(CRFloss_all),accCRF,accIsQA))\n",
    "\n",
    "        if i % 2000 == 0:\n",
    "            print(\"Eval on Devset...\")\n",
    "            accIsQA, accCRF = Eval(model, dev_iter)\n",
    "            if accIsQA * accCRF > best_acc:\n",
    "                best_acc = accIsQA * accCRF\n",
    "                if i>0:\n",
    "                    print(\"Devdata 精度提升 备份模型至{}\".format(hp.model_back))\n",
    "                    model.save_pretrained(hp.model_back)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval(model, iterator):\n",
    "\n",
    "    model.eval()\n",
    "    CRFprediction_all, CRFloss_all, IsQAloss_all, y_CRF_all, IsQA_prediction_all, y_IsQA_all= [],[],[],[],[],[]\n",
    "    final_pred_all = []\n",
    "    for i, batch in enumerate(iterator):\n",
    "        _, tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l = batch\n",
    "        IsQA_prediction, CRFprediction, IsQA_loss, crf_loss, y_2d, y_IsQA_2d  = model.forward(tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l,IsQA_l)\n",
    "\n",
    "        ## CRF\n",
    "        CRFprediction_all.append(CRFprediction)\n",
    "        y_CRF_all.append(y_2d)\n",
    "\n",
    "        ## IsQA\n",
    "        IsQA_prediction_all.append(IsQA_prediction)\n",
    "        y_IsQA_all.append(y_IsQA_2d)\n",
    "\n",
    "        ## 综合预测\n",
    "        # [batch_size,seq_len]\n",
    "        final_pred = torch.LongTensor(np.zeros(CRFprediction.size())).to(\"cuda\")\n",
    "        final_pred[IsQA_prediction.squeeze(dim=-1)==1] = CRFprediction[IsQA_prediction.squeeze(dim=-1)==1]\n",
    "        final_pred_all.append(final_pred)\n",
    "\n",
    "        CRFloss_all.append(crf_loss.to(\"cpu\").item())\n",
    "        IsQAloss_all.append(IsQA_loss.to(\"cpu\").item())\n",
    "\n",
    "\n",
    "    accCRF = result_metric(CRFprediction_all, y_CRF_all)\n",
    "    accIsQA = result_metric(IsQA_prediction_all, y_IsQA_all)\n",
    "    accFinal = result_metric(final_pred_all, y_CRF_all)\n",
    "\n",
    "    print(\"<本次评估结果> IsQA-Loss: {:.3f} CRF-Loss: {:.3f} CRF-Result: accCRF = {:.3f} \"\n",
    "          \"IsQA-Result: accIsQA = {:.3f} Final-Result：accFinal = {:.3f}\".\n",
    "          format(np.mean(IsQAloss_all), np.mean(CRFloss_all), accCRF, accIsQA, accFinal))\n",
    "\n",
    "    return accIsQA, accCRF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'WebQA.v1.0'\n",
    "trainset_path = 'me_train.json'\n",
    "dev_path = 'me_validation.ann.json'\n",
    "test_path = 'me_test.ann.json'\n",
    "mode_path = r'bert-base-chinese'\n",
    "\n",
    "\n",
    "train_dataset = WebQADataset(os.path.join(dir_path ,trainset_path))\n",
    "dev_dataset = WebQADataset(os.path.join(dir_path ,dev_path))\n",
    "test_dataset = WebQADataset(os.path.join(dir_path ,test_path))\n",
    "\n",
    "samples_weight = train_dataset.get_samples_weight(hp.Negweight)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                     batch_size=hp.batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     sampler=sampler,\n",
    "                                     num_workers=0,\n",
    "                                     collate_fn=pad\n",
    "                                     )\n",
    "dev_iter = data.DataLoader(dataset=dev_dataset,\n",
    "                                   batch_size=hp.batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=0,\n",
    "                                   collate_fn=pad\n",
    "                                   )\n",
    "test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=hp.batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=0,\n",
    "                                    collate_fn=pad\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======初始化模型======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnsweringWithCRF: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnsweringWithCRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnsweringWithCRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnsweringWithCRF were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['CRF_fc1.1.weight', 'CRF_fc1.1.bias', 'CRF.transitions', 'fc2.weight', 'fc2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Eval On TestData\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/37371 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<本次评估结果> IsQA-Loss: 0.619 CRF-Loss: 901.782 CRF-Result: accCRF = 0.000 IsQA-Result: accIsQA = 0.733 Final-Result：accFinal = 0.001\n",
      "=========TRAIN and EVAL at epoch=1=========\n",
      "Eval on Devset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 1/37371 [00:45<475:22:49, 45.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<本次评估结果> IsQA-Loss: 0.646 CRF-Loss: 570.588 CRF-Result: accCRF = 0.000 IsQA-Result: accIsQA = 0.667 Final-Result：accFinal = 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                           | 101/37371 [01:50<7:00:31,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-100 IsQA-Loss: 0.667 CRF-Loss: 176.391  CRF-Result: accCRF = 0.035  IsQA-Result: accIsQA = 0.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                           | 201/37371 [02:56<7:17:40,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-200 IsQA-Loss: 0.505 CRF-Loss: 137.478  CRF-Result: accCRF = 0.062  IsQA-Result: accIsQA = 0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                           | 301/37371 [04:00<6:48:55,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-300 IsQA-Loss: 0.412 CRF-Loss: 121.576  CRF-Result: accCRF = 0.087  IsQA-Result: accIsQA = 0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                           | 401/37371 [05:05<7:12:35,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-400 IsQA-Loss: 0.351 CRF-Loss: 111.794  CRF-Result: accCRF = 0.112  IsQA-Result: accIsQA = 0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                           | 501/37371 [06:10<7:30:05,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-500 IsQA-Loss: 0.314 CRF-Loss: 104.357  CRF-Result: accCRF = 0.139  IsQA-Result: accIsQA = 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                          | 601/37371 [07:15<7:11:51,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-600 IsQA-Loss: 0.290 CRF-Loss: 98.636  CRF-Result: accCRF = 0.161  IsQA-Result: accIsQA = 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                          | 701/37371 [08:19<7:43:13,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-700 IsQA-Loss: 0.273 CRF-Loss: 94.103  CRF-Result: accCRF = 0.179  IsQA-Result: accIsQA = 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                          | 801/37371 [09:23<6:52:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-800 IsQA-Loss: 0.261 CRF-Loss: 90.363  CRF-Result: accCRF = 0.197  IsQA-Result: accIsQA = 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                          | 901/37371 [10:28<7:25:56,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-900 IsQA-Loss: 0.250 CRF-Loss: 86.513  CRF-Result: accCRF = 0.219  IsQA-Result: accIsQA = 0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                         | 1000/37371 [11:33<6:42:58,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1000 IsQA-Loss: 0.242 CRF-Loss: 83.563  CRF-Result: accCRF = 0.236  IsQA-Result: accIsQA = 0.927\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.066 CRF-Loss: 42.741 CRF-Result: accCRF = 0.486 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.486\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▏                                                                        | 1101/37371 [13:26<7:51:12,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1100 IsQA-Loss: 0.235 CRF-Loss: 80.781  CRF-Result: accCRF = 0.252  IsQA-Result: accIsQA = 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                        | 1201/37371 [14:34<8:13:01,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1200 IsQA-Loss: 0.229 CRF-Loss: 78.461  CRF-Result: accCRF = 0.265  IsQA-Result: accIsQA = 0.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                        | 1301/37371 [15:38<7:13:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1300 IsQA-Loss: 0.226 CRF-Loss: 76.469  CRF-Result: accCRF = 0.277  IsQA-Result: accIsQA = 0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▊                                                                        | 1401/37371 [16:42<8:09:22,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1400 IsQA-Loss: 0.220 CRF-Loss: 74.367  CRF-Result: accCRF = 0.291  IsQA-Result: accIsQA = 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███                                                                        | 1501/37371 [17:47<7:56:36,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1500 IsQA-Loss: 0.216 CRF-Loss: 72.611  CRF-Result: accCRF = 0.303  IsQA-Result: accIsQA = 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                       | 1601/37371 [18:53<8:40:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1600 IsQA-Loss: 0.210 CRF-Loss: 71.191  CRF-Result: accCRF = 0.312  IsQA-Result: accIsQA = 0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▍                                                                       | 1701/37371 [20:00<8:43:13,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1700 IsQA-Loss: 0.207 CRF-Loss: 69.994  CRF-Result: accCRF = 0.321  IsQA-Result: accIsQA = 0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▌                                                                       | 1801/37371 [21:06<8:23:45,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1800 IsQA-Loss: 0.203 CRF-Loss: 68.775  CRF-Result: accCRF = 0.329  IsQA-Result: accIsQA = 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                       | 1901/37371 [22:12<8:24:07,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-1900 IsQA-Loss: 0.202 CRF-Loss: 67.510  CRF-Result: accCRF = 0.338  IsQA-Result: accIsQA = 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                       | 2000/37371 [23:17<6:24:16,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2000 IsQA-Loss: 0.199 CRF-Loss: 66.417  CRF-Result: accCRF = 0.347  IsQA-Result: accIsQA = 0.945\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.062 CRF-Loss: 32.140 CRF-Result: accCRF = 0.626 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.626\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▏                                                                      | 2101/37371 [25:11<8:40:12,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2100 IsQA-Loss: 0.197 CRF-Loss: 65.453  CRF-Result: accCRF = 0.354  IsQA-Result: accIsQA = 0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▍                                                                      | 2201/37371 [26:18<8:26:33,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2200 IsQA-Loss: 0.195 CRF-Loss: 64.593  CRF-Result: accCRF = 0.361  IsQA-Result: accIsQA = 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                      | 2301/37371 [27:25<8:56:47,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2300 IsQA-Loss: 0.193 CRF-Loss: 63.665  CRF-Result: accCRF = 0.368  IsQA-Result: accIsQA = 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                      | 2401/37371 [28:31<8:49:17,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2400 IsQA-Loss: 0.191 CRF-Loss: 62.854  CRF-Result: accCRF = 0.374  IsQA-Result: accIsQA = 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████                                                                      | 2501/37371 [29:36<8:46:08,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2500 IsQA-Loss: 0.189 CRF-Loss: 61.922  CRF-Result: accCRF = 0.381  IsQA-Result: accIsQA = 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▏                                                                     | 2601/37371 [30:43<8:54:25,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2600 IsQA-Loss: 0.187 CRF-Loss: 61.117  CRF-Result: accCRF = 0.387  IsQA-Result: accIsQA = 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▍                                                                     | 2701/37371 [31:48<9:05:09,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2700 IsQA-Loss: 0.187 CRF-Loss: 60.324  CRF-Result: accCRF = 0.393  IsQA-Result: accIsQA = 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                     | 2801/37371 [32:53<9:00:49,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2800 IsQA-Loss: 0.186 CRF-Loss: 59.729  CRF-Result: accCRF = 0.398  IsQA-Result: accIsQA = 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▊                                                                     | 2901/37371 [33:59<9:23:12,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-2900 IsQA-Loss: 0.184 CRF-Loss: 59.074  CRF-Result: accCRF = 0.404  IsQA-Result: accIsQA = 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████                                                                     | 3000/37371 [35:04<6:19:57,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3000 IsQA-Loss: 0.184 CRF-Loss: 58.487  CRF-Result: accCRF = 0.408  IsQA-Result: accIsQA = 0.952\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.060 CRF-Loss: 28.914 CRF-Result: accCRF = 0.673 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.673\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                    | 3101/37371 [36:59<8:25:02,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3100 IsQA-Loss: 0.184 CRF-Loss: 57.869  CRF-Result: accCRF = 0.413  IsQA-Result: accIsQA = 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▍                                                                    | 3201/37371 [38:07<9:28:21,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3200 IsQA-Loss: 0.183 CRF-Loss: 57.329  CRF-Result: accCRF = 0.417  IsQA-Result: accIsQA = 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▌                                                                    | 3301/37371 [39:14<8:53:01,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3300 IsQA-Loss: 0.182 CRF-Loss: 56.724  CRF-Result: accCRF = 0.422  IsQA-Result: accIsQA = 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▊                                                                    | 3401/37371 [40:19<9:53:42,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3400 IsQA-Loss: 0.181 CRF-Loss: 56.196  CRF-Result: accCRF = 0.426  IsQA-Result: accIsQA = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████                                                                    | 3501/37371 [41:25<9:25:04,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3500 IsQA-Loss: 0.180 CRF-Loss: 55.685  CRF-Result: accCRF = 0.430  IsQA-Result: accIsQA = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▏                                                                   | 3601/37371 [42:32<9:51:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3600 IsQA-Loss: 0.180 CRF-Loss: 55.123  CRF-Result: accCRF = 0.435  IsQA-Result: accIsQA = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▎                                                                  | 3701/37371 [43:38<10:19:28,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3700 IsQA-Loss: 0.180 CRF-Loss: 54.635  CRF-Result: accCRF = 0.439  IsQA-Result: accIsQA = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                   | 3801/37371 [44:46<9:22:13,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3800 IsQA-Loss: 0.179 CRF-Loss: 54.214  CRF-Result: accCRF = 0.442  IsQA-Result: accIsQA = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▊                                                                   | 3901/37371 [45:53<9:59:45,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-3900 IsQA-Loss: 0.179 CRF-Loss: 53.817  CRF-Result: accCRF = 0.446  IsQA-Result: accIsQA = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████                                                                   | 4000/37371 [46:57<6:12:14,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4000 IsQA-Loss: 0.178 CRF-Loss: 53.330  CRF-Result: accCRF = 0.449  IsQA-Result: accIsQA = 0.953\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.059 CRF-Loss: 26.917 CRF-Result: accCRF = 0.706 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.706\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▏                                                                  | 4101/37371 [48:52<9:29:53,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4100 IsQA-Loss: 0.177 CRF-Loss: 52.841  CRF-Result: accCRF = 0.454  IsQA-Result: accIsQA = 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▎                                                                 | 4201/37371 [49:59<10:12:21,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4200 IsQA-Loss: 0.176 CRF-Loss: 52.368  CRF-Result: accCRF = 0.457  IsQA-Result: accIsQA = 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████▌                                                                 | 4301/37371 [51:04<10:01:06,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4300 IsQA-Loss: 0.175 CRF-Loss: 51.950  CRF-Result: accCRF = 0.461  IsQA-Result: accIsQA = 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████▋                                                                 | 4401/37371 [52:11<10:25:49,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4400 IsQA-Loss: 0.174 CRF-Loss: 51.517  CRF-Result: accCRF = 0.465  IsQA-Result: accIsQA = 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████▉                                                                 | 4501/37371 [53:18<10:38:05,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4500 IsQA-Loss: 0.173 CRF-Loss: 51.121  CRF-Result: accCRF = 0.469  IsQA-Result: accIsQA = 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▏                                                                 | 4601/37371 [54:25<9:52:45,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4600 IsQA-Loss: 0.171 CRF-Loss: 50.801  CRF-Result: accCRF = 0.472  IsQA-Result: accIsQA = 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████▎                                                                | 4701/37371 [55:33<10:25:30,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4700 IsQA-Loss: 0.171 CRF-Loss: 50.489  CRF-Result: accCRF = 0.474  IsQA-Result: accIsQA = 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████▋                                                                 | 4801/37371 [56:41<9:56:56,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4800 IsQA-Loss: 0.171 CRF-Loss: 50.169  CRF-Result: accCRF = 0.477  IsQA-Result: accIsQA = 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████▋                                                                | 4901/37371 [57:48<10:31:04,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-4900 IsQA-Loss: 0.170 CRF-Loss: 49.769  CRF-Result: accCRF = 0.480  IsQA-Result: accIsQA = 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████                                                                 | 5000/37371 [58:52<6:11:39,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5000 IsQA-Loss: 0.170 CRF-Loss: 49.456  CRF-Result: accCRF = 0.483  IsQA-Result: accIsQA = 0.956\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.059 CRF-Loss: 26.119 CRF-Result: accCRF = 0.713 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.713\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████▊                                                              | 5101/37371 [1:00:49<10:51:54,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5100 IsQA-Loss: 0.169 CRF-Loss: 49.122  CRF-Result: accCRF = 0.486  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████                                                              | 5201/37371 [1:01:57<10:37:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5200 IsQA-Loss: 0.168 CRF-Loss: 48.813  CRF-Result: accCRF = 0.489  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▏                                                             | 5301/37371 [1:03:02<10:40:44,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5300 IsQA-Loss: 0.168 CRF-Loss: 48.515  CRF-Result: accCRF = 0.491  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▍                                                             | 5401/37371 [1:04:09<10:42:30,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5400 IsQA-Loss: 0.167 CRF-Loss: 48.255  CRF-Result: accCRF = 0.494  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████▌                                                             | 5501/37371 [1:05:16<11:03:47,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5500 IsQA-Loss: 0.166 CRF-Loss: 47.938  CRF-Result: accCRF = 0.496  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████▊                                                             | 5601/37371 [1:06:22<11:15:29,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5600 IsQA-Loss: 0.166 CRF-Loss: 47.626  CRF-Result: accCRF = 0.499  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████▉                                                             | 5701/37371 [1:07:29<11:01:29,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5700 IsQA-Loss: 0.165 CRF-Loss: 47.305  CRF-Result: accCRF = 0.502  IsQA-Result: accIsQA = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▏                                                            | 5801/37371 [1:08:34<10:03:29,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5800 IsQA-Loss: 0.164 CRF-Loss: 47.004  CRF-Result: accCRF = 0.504  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▎                                                            | 5901/37371 [1:09:41<10:39:25,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-5900 IsQA-Loss: 0.164 CRF-Loss: 46.772  CRF-Result: accCRF = 0.506  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▋                                                             | 6000/37371 [1:10:45<5:37:29,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6000 IsQA-Loss: 0.163 CRF-Loss: 46.499  CRF-Result: accCRF = 0.509  IsQA-Result: accIsQA = 0.957\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.053 CRF-Loss: 24.179 CRF-Result: accCRF = 0.738 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.738\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▊                                                            | 6101/37371 [1:12:44<11:27:51,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6100 IsQA-Loss: 0.163 CRF-Loss: 46.232  CRF-Result: accCRF = 0.511  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████▉                                                            | 6201/37371 [1:13:52<11:21:15,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6200 IsQA-Loss: 0.162 CRF-Loss: 45.991  CRF-Result: accCRF = 0.513  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████▏                                                           | 6301/37371 [1:14:59<11:38:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6300 IsQA-Loss: 0.162 CRF-Loss: 45.729  CRF-Result: accCRF = 0.515  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████▎                                                           | 6401/37371 [1:16:08<11:36:05,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6400 IsQA-Loss: 0.162 CRF-Loss: 45.480  CRF-Result: accCRF = 0.518  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████▌                                                           | 6501/37371 [1:17:16<12:08:23,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6500 IsQA-Loss: 0.161 CRF-Loss: 45.234  CRF-Result: accCRF = 0.520  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████▋                                                           | 6601/37371 [1:18:23<11:43:17,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6600 IsQA-Loss: 0.161 CRF-Loss: 44.976  CRF-Result: accCRF = 0.522  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████▉                                                           | 6701/37371 [1:19:31<11:32:19,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6700 IsQA-Loss: 0.161 CRF-Loss: 44.726  CRF-Result: accCRF = 0.524  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████                                                           | 6801/37371 [1:20:39<11:31:42,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6800 IsQA-Loss: 0.160 CRF-Loss: 44.487  CRF-Result: accCRF = 0.526  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▎                                                          | 6901/37371 [1:21:46<11:52:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-6900 IsQA-Loss: 0.160 CRF-Loss: 44.262  CRF-Result: accCRF = 0.528  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▋                                                           | 7000/37371 [1:22:50<5:05:05,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7000 IsQA-Loss: 0.159 CRF-Loss: 44.064  CRF-Result: accCRF = 0.530  IsQA-Result: accIsQA = 0.957\n",
      "Eval on Devset...\n",
      "<本次评估结果> IsQA-Loss: 0.058 CRF-Loss: 23.928 CRF-Result: accCRF = 0.742 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.742\n",
      "Devdata 精度提升 备份模型至save_model/back_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▋                                                          | 7101/37371 [1:24:47<11:19:29,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7100 IsQA-Loss: 0.159 CRF-Loss: 43.892  CRF-Result: accCRF = 0.531  IsQA-Result: accIsQA = 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▊                                                          | 7201/37371 [1:25:54<11:56:40,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7200 IsQA-Loss: 0.158 CRF-Loss: 43.686  CRF-Result: accCRF = 0.533  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████                                                          | 7301/37371 [1:27:02<11:50:34,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7300 IsQA-Loss: 0.157 CRF-Loss: 43.478  CRF-Result: accCRF = 0.535  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▎                                                         | 7401/37371 [1:28:07<11:30:37,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7400 IsQA-Loss: 0.157 CRF-Loss: 43.246  CRF-Result: accCRF = 0.537  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▍                                                         | 7501/37371 [1:29:15<13:23:39,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7500 IsQA-Loss: 0.156 CRF-Loss: 43.065  CRF-Result: accCRF = 0.539  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▋                                                         | 7601/37371 [1:30:22<12:17:30,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7600 IsQA-Loss: 0.156 CRF-Loss: 42.865  CRF-Result: accCRF = 0.540  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████████▊                                                         | 7701/37371 [1:31:31<12:01:02,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7700 IsQA-Loss: 0.156 CRF-Loss: 42.667  CRF-Result: accCRF = 0.542  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████                                                         | 7801/37371 [1:32:42<13:11:58,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7800 IsQA-Loss: 0.156 CRF-Loss: 42.489  CRF-Result: accCRF = 0.544  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████▏                                                        | 7901/37371 [1:33:51<11:54:49,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-7900 IsQA-Loss: 0.155 CRF-Loss: 42.312  CRF-Result: accCRF = 0.546  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████▋                                                         | 8000/37371 [1:34:53<4:51:19,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8000 IsQA-Loss: 0.155 CRF-Loss: 42.143  CRF-Result: accCRF = 0.547  IsQA-Result: accIsQA = 0.958\n",
      "Eval on Devset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████▏                                                       | 8001/37371 [1:35:43<124:25:39, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<本次评估结果> IsQA-Loss: 0.060 CRF-Loss: 22.856 CRF-Result: accCRF = 0.737 IsQA-Result: accIsQA = 0.992 Final-Result：accFinal = 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████▌                                                        | 8101/37371 [1:36:49<11:32:20,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8100 IsQA-Loss: 0.155 CRF-Loss: 41.979  CRF-Result: accCRF = 0.549  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████▊                                                        | 8201/37371 [1:38:02<13:21:24,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8200 IsQA-Loss: 0.154 CRF-Loss: 41.803  CRF-Result: accCRF = 0.551  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████▉                                                        | 8301/37371 [1:39:13<12:07:15,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8300 IsQA-Loss: 0.154 CRF-Loss: 41.626  CRF-Result: accCRF = 0.552  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▏                                                       | 8401/37371 [1:40:22<12:44:02,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8400 IsQA-Loss: 0.154 CRF-Loss: 41.477  CRF-Result: accCRF = 0.554  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████▍                                                       | 8501/37371 [1:41:31<11:47:26,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8500 IsQA-Loss: 0.153 CRF-Loss: 41.288  CRF-Result: accCRF = 0.556  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████▌                                                       | 8601/37371 [1:42:39<12:40:48,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Last 100 Steps MeanValue> Setp-8600 IsQA-Loss: 0.153 CRF-Loss: 41.143  CRF-Result: accCRF = 0.557  IsQA-Result: accIsQA = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████▊                                                        | 8626/37371 [1:42:55<5:43:00,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-93bc501f3389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"=========TRAIN and EVAL at epoch={epoch}=========\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mTrainOneEpoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# print(f\"=========eval dev at epoch={epoch}=========\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-a6154ef7c113>\u001b[0m in \u001b[0;36mTrainOneEpoch\u001b[1;34m(model, train_iter, dev_iter, test_iter, optimizer, hp)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_id_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_offset_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_seq_label_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIsQA_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mIsQA_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCRFprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIsQA_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrf_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_IsQA_2d\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_id_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_offset_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_seq_label_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIsQA_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m## CRF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mCRFprediction_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCRFprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-053ebaad84eb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tokens_id_l, token_type_ids_l, answer_offset_l, answer_seq_label_l, IsQA_l)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# No [CLS]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mcrf_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCRF_fc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mcrf_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_log_likelihood_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrf_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_2d\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCRFprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrf_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-07fa554e316c>\u001b[0m in \u001b[0;36mneg_log_likelihood_loss\u001b[1;34m(self, feats, mask, tags)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \"\"\"\n\u001b[0;32m    252\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mforward_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_alg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[0mgold_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mforward_score\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgold_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-07fa554e316c>\u001b[0m in \u001b[0;36m_forward_alg\u001b[1;34m(self, feats, mask)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \"\"\"\n\u001b[0;32m     84\u001b[0m             \u001b[0mcur_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_values\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mcur_partition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mmask_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-07fa554e316c>\u001b[0m in \u001b[0;36mlog_sum_exp\u001b[1;34m(vec, m_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmax_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# B * M\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     return max_score.view(-1, m_size) + torch.log(torch.sum(\n\u001b[1;32m---> 22\u001b[1;33m         torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(hp.model_path):\n",
    "    print('=======载入模型=======')\n",
    "    model = torch.load(hp.model_path)\n",
    "else:\n",
    "    print(\"=======初始化模型======\")\n",
    "    model = BertForQuestionAnsweringWithCRF.from_pretrained(mode_path )\n",
    "    if hp.device == 'cuda':\n",
    "        model = model.cuda()\n",
    "    #model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp.lr, weight_decay=hp.l2)\n",
    "\n",
    "if not os.path.exists(os.path.split(hp.model_path)[0]):\n",
    "    os.makedirs(os.path.split(hp.model_path)[0])\n",
    "\n",
    "print(\"First Eval On TestData\")\n",
    "accIsQA, accCRF = Eval(model, test_iter)\n",
    "\n",
    "best_acc = max(0, accIsQA*accCRF )\n",
    "no_gain_rc = 0    #效果不增加代数\n",
    "\n",
    "for epoch in range(1, hp.n_epochs + 1):\n",
    "    print(f\"=========TRAIN and EVAL at epoch={epoch}=========\")\n",
    "    TrainOneEpoch(model, train_iter, dev_iter,test_iter, optimizer, hp)\n",
    "\n",
    "    # print(f\"=========eval dev at epoch={epoch}=========\")\n",
    "    # dev_acc = eval(model, dev_iter)\n",
    "    print(f\"=========eval test at epoch={epoch}=========\")\n",
    "    accIsQA, accCRF = Eval(model, test_iter)\n",
    "\n",
    "    if accIsQA*accCRF >best_acc:\n",
    "        print(\"精度值由 {:.3f} 更新至 {:.3f} \".format(best_acc, accIsQA*accCRF))\n",
    "        best_acc = accIsQA*accCRF\n",
    "        print(\"=======保存模型=======\")\n",
    "        torch.save(model, hp.model_path)\n",
    "        no_gain_rc = 0\n",
    "    else:\n",
    "        no_gain_rc = no_gain_rc+1\n",
    "\n",
    "        # 提前终止\n",
    "    if no_gain_rc > hp.early_stop:\n",
    "        print(\"连续{}个epoch没有提升，在epoch={}提前终止\".format(no_gain_rc,epoch))\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict(self,tokens_id_l, token_type_ids_l):\n",
    "        tokens_x_2d = torch.LongTensor(tokens_id_l).to(self.device)\n",
    "        token_type_ids_2d = torch.LongTensor(token_type_ids_l).to(self.device)\n",
    "\n",
    "        batch_size, seq_length = tokens_x_2d[:,1:].size()\n",
    "        self.PreModel.eval()\n",
    "        with torch.no_grad():\n",
    "            emb, _ = self.PreModel(tokens_x_2d,token_type_ids=token_type_ids_2d)\n",
    "\n",
    "        ## [CLS] for IsQA  [batch_size, hidden_size]\n",
    "        cls_emb = emb[:,0,:]\n",
    "        ## [batch_size, 2]\n",
    "        IsQA_logits = self.fc2(cls_emb)\n",
    "        ## [batch_size, 1]\n",
    "        IsQA_prediction = IsQA_logits.argmax(dim=-1)\n",
    "\n",
    "        # CRF mask\n",
    "        mask = np.ones(shape=[batch_size, seq_length], dtype=np.uint8)\n",
    "        mask = torch.ByteTensor(mask).to(self.device)\n",
    "        # [batch_size, seq_len, 4]\n",
    "        crf_logits = self.CRF_fc1(emb[:,1:,:])\n",
    "        _, CRFprediction = self.CRF.forward(feats=crf_logits, mask=mask)\n",
    "\n",
    "        return IsQA_prediction.to(\"cpu\"), CRFprediction.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Demo(model, q, e):\n",
    "    tokens = tokenizer.tokenize('[CLS]' + q + '[SEP]' + e)  # list\n",
    "    if len(tokens) > 512:\n",
    "        tokens = tokens[:512]\n",
    "    tokens_id = [tokenizer.convert_tokens_to_ids(tokens)]#[[101,...,102,...]]\n",
    "    token_type_ids = [0 if i <= tokens_id[0].index(102) else 1 for i in range(len(tokens_id))]\n",
    "    IsQA_prediction, CRFprediction = model.module.predict(tokens_id,token_type_ids)\n",
    "    CRFprediction = CRFprediction.numpy()[0]\n",
    "    IsQA_prediction = IsQA_prediction.numpy()[0]\n",
    "    answer = \"\"\n",
    "    if IsQA_prediction==1:\n",
    "        for i in range(len(tokens[1:])):\n",
    "            if CRFprediction[i].item()==1:\n",
    "                answer = answer + tokens[1:][i]\n",
    "    return answer\n",
    "\n",
    "def prepare_knowledge(knowledge_path,Stopword_path):\n",
    "    def del_stopword(line, Stopword, ngram=False):\n",
    "        line = list(jieba.cut(line))\n",
    "        new = [word for word in line if word not in Stopword]\n",
    "        if ngram:  # 返回2元语法\n",
    "            N = len(line)\n",
    "            for i, word_i in enumerate(line):\n",
    "                for j in range(min(i + 1, N - 1), N):\n",
    "                    word_j = line[j]\n",
    "                    if word_i not in Stopword and word_j not in Stopword:\n",
    "                        new.append(word_i + ' ' + word_j)\n",
    "        return new  # [w1,w2,...]\n",
    "    print(\"正在准备知识库...\")\n",
    "    dataset = Knowledge(knowledge_path)\n",
    "    with open(Stopword_path, \"r\", encoding=\"gbk\") as f:\n",
    "        Stopword = set(f.read().splitlines())\n",
    "    documents = dataset.evidences\n",
    "    corpus = [\" \".join(del_stopword(e, Stopword)) for e in documents]\n",
    "    vectorizer = CountVectorizer()  # ngram_range=(1,2)\n",
    "    count = vectorizer.fit_transform(corpus)\n",
    "    # 计算TF-IDF向量\n",
    "    TFIDF = TfidfTransformer()\n",
    "    tfidf_matrix = TFIDF.fit_transform(count)\n",
    "    d_matrix = np.array(tfidf_matrix.toarray())\n",
    "    vocabulary_ = vectorizer.vocabulary_\n",
    "    return d_matrix, vocabulary_, del_stopword, Stopword, dataset\n",
    "\n",
    "def QA(model, question, xu, knowledge):\n",
    "    # 按相关度从大到小 #[1,2,3,...]\n",
    "    xu.reverse()\n",
    "    xu = [item[0] for item in xu]\n",
    "    ## 有序字典 按相关度从大到小插入key\n",
    "    result =collections.OrderedDict()\n",
    "    q = question\n",
    "    tokens_id_l = []\n",
    "    token_type_ids_l = []\n",
    "    tokens_l = []\n",
    "    for index in xu:\n",
    "        e=knowledge.evidences[index]\n",
    "        tokens = tokenizer.tokenize('[CLS]' + q + '[SEP]' + e)  # list\n",
    "        if len(tokens) > 512:\n",
    "            tokens = tokens[:512]\n",
    "        tokens_id = tokenizer.convert_tokens_to_ids(tokens)  # [101,...,102,...]\n",
    "        token_type_ids = [0 if i <= tokens_id.index(102) else 1 for i in range(len(tokens_id))]\n",
    "        tokens_id_l.append(tokens_id)\n",
    "        token_type_ids_l.append(token_type_ids)\n",
    "        tokens_l.append(tokens[1:])\n",
    "    ## pad\n",
    "    max_len = max([len(x) for x in tokens_id_l])\n",
    "    tokens_id_l = [x+(max_len - len(x))*tokenizer.convert_tokens_to_ids(['[PAD]']) for x in tokens_id_l ]\n",
    "    token_type_ids_l = [x+(max_len - len(x))*[1] for x in token_type_ids_l ]\n",
    "\n",
    "    ## 批预测\n",
    "    IsQA_prediction, CRFprediction = model.module.predict(tokens_id_l,token_type_ids_l)\n",
    "    CRFprediction = CRFprediction.numpy() #[batch_size, max_len]\n",
    "    IsQA_prediction = IsQA_prediction.numpy()#[batch_size, 1]\n",
    "    for k in range(len(xu)):\n",
    "        answer = \"\"\n",
    "        tokens = tokens_l[k]\n",
    "        if IsQA_prediction[k]==1:#[cls]判断\n",
    "            for i in range(len(tokens)):\n",
    "                if CRFprediction[k,i].item()==1:\n",
    "                    answer = answer + tokens[i]\n",
    "        # 记录answer\n",
    "        if answer in result:\n",
    "            result[answer] = result[answer] + 1\n",
    "        else:\n",
    "            if answer:\n",
    "                result[answer] = 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(hp.model_path)\n",
    "            ques_num=1\n",
    "            while True:\n",
    "                print(\"请输入问题-{}:\".format(ques_num))\n",
    "                question = input()\n",
    "                if question == \"OVER\":\n",
    "                    print(\"问答结束！\")\n",
    "                    break\n",
    "                print(\"请输入文章：\")\n",
    "                evidence = input()\n",
    "                # print(\"正在解析...\")\n",
    "                start = time.time()\n",
    "                answer = Demo(model,question,evidence)\n",
    "                end = time.time()\n",
    "                if answer:\n",
    "                    print(\"问题-{}的答案是：{}\".format(ques_num,answer))\n",
    "                    print(\"耗时:{:.2f}毫秒\".format((end-start)*1e3))\n",
    "                else:\n",
    "                    print(\"文章中没有答案\")\n",
    "                ques_num = ques_num + 1\n",
    "        else:\n",
    "            print(\"没有可用模型！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(hp.model_path)\n",
    "            ques_num=1\n",
    "            #准备知识库\n",
    "            d_matrix, vocabulary_, del_stopword, Stopword, knowledge = prepare_knowledge(hp.knowledge_path,hp.Stopword_path)\n",
    "            while True:\n",
    "                # try:\n",
    "                print(\"请输入问题-{}:\".format(ques_num))\n",
    "                question = input()\n",
    "                if question == \"OVER\":\n",
    "                    print(\"问答结束！\")\n",
    "                    break\n",
    "\n",
    "                # 创建问句tf-idf向量\n",
    "                q_vector = np.zeros([1, d_matrix.shape[1]])\n",
    "                q_list = del_stopword(question,Stopword,ngram=False)\n",
    "                for word in q_list:\n",
    "                    if word in vocabulary_:\n",
    "                        q_vector[0,vocabulary_[word]] = 1.\n",
    "                dot = (np.mat(d_matrix))*(np.mat(q_vector.T))\n",
    "                xu=dot.argsort(0)[-15:].tolist()# [[12], [37], [10]] 最大的15个索引\n",
    "\n",
    "                start = time.time()\n",
    "                answer = QA(model, question, xu, knowledge)\n",
    "                end = time.time()\n",
    "\n",
    "                print(\"问题-{}的答案是:\".format(ques_num))\n",
    "                for ai,a in enumerate(answer):\n",
    "                    print(\"推荐答案No.{}: {}\".format(ai+1,a))\n",
    "                print(\"耗时:{:.2f}毫秒\".format((end-start)*1e3))\n",
    "\n",
    "                ques_num = ques_num + 1\n",
    "        else:\n",
    "            print(\"没有可用模型！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料：https://github.com/Hanlard/Bert-for-WebQA/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
