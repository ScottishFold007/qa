{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理流程简单说明\n",
    "- 文档预处理使用句子组合的形式，在最大可容纳字长中包含连续的多个句子，考虑到答案是连续且完整的句子组合\n",
    "- 在正负样本的选择中，选取了所有的正样本和随机一个负样本进行训练\n",
    "- 训练过程使用多任务方式，同时预测答案是否在当前片段内以及答案的起始位置，这两个loss按照合适的权重相加\n",
    "- 预测过程中，样本的每个token都会输出一个start&end的logit，各自选择logit最大的前k个，排列组合为备选span，然后计算span-score：startlogit+endlogit-cls_start-cls_end，按照- spanscore对预测样本进行排序，选择分值最大的span最为最终答案\n",
    "- 之后提分的思路可以从检索方面提高检索准确率，从重排序的分值计算方式和排序方式，以及多任务的训练方式上面提高单模的性能，使用更合适的预训练语言模型也能直接提升效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Features, Value, ClassLabel, load_dataset\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "from model import BertForQuestionAnsweringWithMultiTask\n",
    "from utils import  Prepare_Train_Features,show_random_elements,Prepare_Train_Features_For_CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/default-16fd39ceb5503ccf (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to squad_chinese\\squad\\default-16fd39ceb5503ccf\\0.0.0\\6f2fe9bd41d2e840220525d0b40a863add682e883599aca2de6940b809fa66a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74d7500e8ad466a911651acad745552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6890f767b8d048c38e0fce25ad4e4485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to squad_chinese\\squad\\default-16fd39ceb5503ccf\\0.0.0\\6f2fe9bd41d2e840220525d0b40a863add682e883599aca2de6940b809fa66a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(r\"C:\\Users\\hp\\Desktop\\2021.02.24 BertForQuestionAnsweringWithMultiTask\\squad_chinese\")\n",
    " \n",
    "features = Features({'answers': Value('string'), 'context': Value('string'), 'id': Value('string'), 'question': Value('string'), 'title': Value('string')})\n",
    "\n",
    "file_dict = {'train': PATH/'train-zen-v1.0.json','dev': PATH/'dev-zen-v1.0.json'} #载入训练集和测试集\n",
    "\n",
    "datasets = load_dataset(\n",
    "                       path = r\"C:\\Users\\hp\\Desktop\\2021.02.24 BertForQuestionAnsweringWithMultiTask\\squad_chinese\", \n",
    "                       data_files=file_dict, \n",
    "                       script_version='master', \n",
    "                       #split='train',\n",
    "                       cache_dir=r\"squad_chinese\",\n",
    "                       \n",
    "                         \n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4997\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4997\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': {'answer_start': [41],\n",
       "  'text': ['省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委']},\n",
       " 'context': '福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委联合下发通知，出台一系列暖企措施支持疫情防控期间复工稳岗。 通知明确，切实发挥各级农民工工作领导小组办公室的统筹协调作用, 加强劳务用工有效对接，对具备外出务工条件、可成规模输送到我省用工地，并在出行前14天内及在途没有相关症状的，可由用工地和输出地联合开展“点对点、一站式”直达企业的专门运输。省级公共就业服务机构可与主要劳务输出省份签订劳务协作协议、设立劳务协作工作站，对每个工作站给予一次性10万元就业服务经费补助。鼓励优先聘用本地劳务人员。 未经省应对新冠肺炎疫情工作有关机构确认的疫情防控急需物资生产企业引进劳动力的，一次性用工服务奖补标准最高提到每人2000元。对上述企业坚持在生产一线工作的职工，给予每人每天100元的生活补助，纳入一次性用工服务奖补范畴。对春节当月至疫情一级响应结束月，采取稳定职工队伍保持连续生产的企业，给予一次性稳就业奖补。 加大失业保险稳岗返还力度，将中小微企业稳岗返还政策裁员率标准调整为不高于上年度全国调查失业率的控制目标，对参保职工30人（含）以下的企业，裁员率调整为不超过企业参保职工总数的20%。对不裁员或少裁员，符合条件的参保企业，可返还其上年度实际缴纳失业保险费的50%。对受疫情影响面临暂时性生产经营困难且恢复有望、坚持不裁员或少裁员、符合条件的参保企业，按6个月的当地月人均失业保险金和参保职工人数落实失业保险稳岗返还政策。 加强职业技能培训，鼓励技工院校学生在符合疫情防控条件下参加实习实训，探索简易岗前技能培训。对企业因生产急需新录用的人员，按每人200元标准一次性给予企业简易岗前技能培训补贴。鼓励实施线上培训，对受疫情影响的企业，在停工期、恢复期组织职工参加各类线上或线下职业培训的，可按规定纳入补贴类培训范围。 通知要求，各地要着力提升政策措施的精准度和有效性，提升各类企业享受政策措施的获得感。各类企业要落实落细防控主体责任，严格落实返岗信息登记、班车错峰接送、员工分散用餐、体温监测等具体应对措施，确保复工稳岗和疫情防控两不误。（记者 潘园园）',\n",
       " 'id': 'edd1413c78e534afb136f36fdc9c9a00',\n",
       " 'question': '福建联合出台暖企措施支持复工稳岗的部门都有谁？',\n",
       " 'title': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'answer_start': [1221], 'text': ['1个月房租免收、2个月租金减半。']}</td>\n",
       "      <td>江西省人民政府印发关于有效应对疫情稳定经济增长20条政策措施的通知 各市、县（区）人民政府，赣江新区管委会，省政府各部门： 现将《关于有效应对疫情稳定经济增长20条政策措施》印发给你们，请认真贯彻执行。 2020年2月4日 （此件主动公开） 关于有效应对疫情稳定经济增长20条政策措施 为深入贯彻习近平总书记关于坚决打赢疫情防控阻击战的重要指示精神，全面落实党中央、国务院有关决策部署，在全力做好疫情防控工作的同时，着力促进全省经济平稳增长，现提出以下政策措施： 一、加强对疫情防控物资和生活必需品生产企业的扶持 1.支持全国性商业银行、国家开发银行、农业发展银行、进出口银行等在赣分支机构加大服务对接力度，全力满足疫情防控融资需求。实行疫情防控重点企业融资白名单制，支持江西银行、九江银行和进贤农商行利用专项再贷款为企业提供优惠利率信贷支持，最高不得超过最近公布的一年期lpr减100个基点。（省金融监管局、人行南昌中心支行牵头，江西银保监局、省工业和信息化厅、省发展改革委等配合） 2.对2020年新增的全省疫情防控重点保障企业专项再贷款，在人民银行专项再贷款支持金融机构提供优惠利率信贷、中央财政按人民银行再贷款利率的50%给予贴息的基础上，省财政统筹资金再给予25%的贴息支持，贴息期限不超过1年。对疫情防控重点保障企业给予稳岗补贴和创业担保贷款支持。对在疫情防控、生活必需品保供稳价工作中主动让利的重点企业和商户，各地可从价格调节基金或其他可用财力中给予一定补助，在项目安排等扶持政策上给予倾斜。（省财政厅、省人力资源社会保障厅、省发展改革委牵头，省工业和信息化厅、人行南昌中心支行，各市、县〔区〕人民政府、赣江新区管委会配合） 3.积极帮助疫情防控重点物资和生活必需品生产企业复工复产，安排专人进行“一对一”蹲点帮扶，协调解决设备、原辅料、人工、资金、运输及用能等实际困难。对扩大疫情防控重点物资产能的企业，经主管部门同意后，可先扩产再补办相关审批手续。疫情防控重点物资生产企业扩大产能、改造生产线发生的实际投入，纳入省级企业技术改造项目支持。（省工业和信息化厅、省发展改革委牵头，省人力资源社会保障厅、省卫生健康委、省财政厅等配合） 4.全省药品补充申请、再注册收费标准和二类医疗器械首次注册、变更注册、延续注册收费标准降低30%。（省发展改革委牵头，省财政厅、省市场监管局等配合） 5.严格落实“一断三不断”要求，稳妥处置未经批准擅自设卡拦截、断路阻碍交通等行为，确保疫情防控物资和必要的生产生活物资运输通畅。简化绿色通道查验手续和程序。对于疫情防控应急物资、由省新型冠状病毒感染的肺炎疫情防控应急指挥部或有关成员单位统一调拨转运的重要生活物资等保障车辆，疫情期间免除高速公路通行费用。（省交通运输厅、省公安厅牵头，各市、县〔区〕人民政府、赣江新区管委会配合） 二、扶持实体企业渡难关 6.对承租国有资产类生产经营用房的企业，1个月房租免收、2个月租金减半。对租用其他经营用房的，鼓励业主为租户减免租金，具体由双方协商解决。对在疫情期间为承租的中小企业减免租金的创业园、科技企业孵化器、创业孵化基地等各类载体，优先予以政策扶持。（省财政厅、省国资委牵头，省工业和信息化厅、省科技厅、省人力资源社会保障厅，各市、县〔区〕人民政府、赣江新区管委会配合） 7.对受疫情影响较大的批发零售、住宿餐饮、物流运输、文化旅游等行业企业和未能及时充分复工的工业企业，及时辅导落实好小微企业普惠性减税等政策。对因疫情原因导致企业发生重大损失，正常生产经营活动受到重大影响，纳税确有困难的，依法予以减免房产税、城镇土地使用税。（省税务局牵头，各市、县〔区〕人民政府、赣江新区管委会配合） 8.对按月申报的纳税人、扣缴义务人，将2020年2月份的法定申报纳税期限延长至2月24日；在申报纳税期限延长后，办理仍有困难的，可依法申请进一步延期，延期期间不征收滞纳金。纳税人受疫情影响确有特殊困难不能按期缴纳税款的，还可依法申请办理延期缴纳税款，最长不超过3个月。（省税务局牵头） 9.加大对企业的金融支持，确保2020年企业信贷余额不低于2019年同期余额。鼓励金融机构适当下调贷款利率，增加信用贷款和中长期贷款额度。引导中小微企业通过江西省小微客户融资服务平台、一站式金融综合服务平台申请贷款。对受疫情影响较大的批发零售、住宿餐饮、物流运输、文化旅游、“三农”领域等行业，以及有发展前景但受疫情影响暂时受困的企业，金融机构不得盲目抽贷、断贷、压贷；对到期还款困难的企业，应予以展期或续贷。（省金融监管局牵头，人行南昌中心支行、江西银保监局等配合） 10.各级政府性融资担保再担保机构要加强与银行机构合作，针对受疫情影响严重行业和疫情防控行业定制担保产品，对因疫情暂遇困难企业特别是小微企业，取消反担保抵质押要求，降低担保费。对受疫情影响严重地区的融资担保机构，省融资担保股份有限公司及各设区市再担保机构减半收取再担保费。（省金融监管局牵头，人行南昌中心支行、江西银保监局，各市、县〔区〕人民政府、赣江新区管委会配合） 11.充分发挥国有大中型企业的中坚作用，依法依规在货款回收、原材料供应、项目发包等方面，加大对产业链上中小企业的支持，确保产业链运行平稳。（省国资委牵头，省工业和信息化厅，各市、县〔区〕人民政府、赣江新区管委会配合） 三、以扩投资为重点稳需求 12.积极推广不见面招商，充分利用赣服通、政务网、公众号等平台，高频次、高精度、大范围进行招商引资项目推介。对成熟且有签约意向的项目，要加强网上对接、洽谈力度，确保尽快签约。对已签约项目，要全力做好项目的立项、开工、投产全过程服务，确保项目尽快落地。（省商务厅牵头，各市、县〔区〕人民政府、赣江新区管委会配合） 13.充分发挥全省投资项目在线审批监管平台作用，全面推广网上收件、网上审批和网上出件。对按规定确需提交纸质材料原件的，除特殊情况外，由项目单位通过在线平台或电子邮件提供电子材料后先行办理；项目单位应对提供的电子材料真实性负责，待疫情结束后补交纸质材料原件。（省发展改革委牵头，省住房城乡建设厅、省自然资源厅、省生态环境厅、省水利厅，各市、县〔区〕人民政府、赣江新区管委会配合）。 14.将省重点工程建设单位人员疫情防控、生活物资保障、施工物资供应等工作纳入地方保供范围，切实协调解决项目建设涉及的市政配套、水电接入、资金落实等问题。对项目建设中确因受疫情影响或疫情防控工作需要不能按时履行合同的，允许合理延后合同执行期限，不作违约处理。（省发展改革委牵头，省直有关部门，各市、县〔区〕人民政府、赣江新区管委会配合） 15.对确因受疫情影响不能及时复产履约的外贸企业，指导企业向国家有关部门申请“新型冠状病毒感染的肺炎疫情不可抗力事实证明”，及时帮助企业最大限度减小损失。设立进出口商品绿色通道，确保进出口商品快速通关。鼓励中国信保江西分公司为因疫情遇到困难的出口企业提供风险保障、保单融资等服务。（省商务厅、南昌海关牵头） 16.支持传统商贸主体电商化、数字化改造升级，积极培育网络诊疗、在线办公、在线教育、线上文化娱乐、影视及智能家居等新兴消费业态和消费热点，繁荣“宅经济”。大力实施“互联网+”农产品出村进城工程，加快城乡商品要素流动。（省商务厅牵头，省发展改革委、省工业和信息化厅、省卫生健康委、省文化和旅游厅、省教育厅、省农业农村厅等配合） 四、加大企业稳岗和就业促进力度 17.鼓励受疫情影响企业与职工协商采取调整薪酬（生活费）、轮岗轮休、缩短工时、待岗等方式稳定工作岗位。对不裁员或少裁员的参保企业，可返还其上年度实际缴纳失业保险费的50％。2020年1月1日至2020年12月31日，对面临暂时性生产经营困难、坚持不裁员或少裁员的参保企业，返还标准按上年度6个月的统筹地月人均失业保险金和企业上年度月均参保职工人数确定。所需资金从失业保险基金列支。（省人力资源社会保障厅、省财政厅牵头） 18.对受疫情影响的企业、灵活就业人员和城乡居民，未能按时办理社会保险缴费业务的，可延长至疫情解除后补办。逾期缴纳社会保险费期间，免收滞纳金，不影响个人权益。相关补办手续在疫情解除后三个月内完成。（省人力资源社会保障厅牵头，省医保局、省财政厅等配合） 19.对已发放的个人创业担保贷款，受疫情影响还款出现困难的，可向贷款银行申请展期还款，展期期限原则上不超过1年，省财政继续给予贴息支持；对受疫情影响未能按时完成展期手续的，免于信用惩戒。对受疫情影响暂时失去收入来源的个人和小微企业，各有关部门要在其申请创业担保贷款时优先给予支持。（省人力资源社会保障厅牵头，省财政厅、省金融监管局、人行南昌中心支行、省发展改革委等配合） 20.密切关注全省农民工、应届毕业生等重点群体就业状况和省内外用工需求，充分发挥人力资源市场网络平台作用，及时发布就业信息、企业开复工信息，开展网上招聘。建立返乡务工人员滞留省内就业应对机制，促进与用工企业精准对接。（省人力资源社会保障厅牵头，省发展改革委、省商务厅、省工业和信息化厅，各市、县〔区〕人民政府、赣江新区管委会配合）</td>\n",
       "      <td>762fdb59ef70306fb8c28171b429ef29</td>\n",
       "      <td>江西省对承租国有资产类生产经营用房的企业有何政策？</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = r'chinese-bert-wwm-ext'\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(''.join(list(set(datasets[\"train\"][\"context\"])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocabs = ['’','‘','“','”']\n",
    "for i in chars:\n",
    "    if i not in base_tokenizer.vocab.keys():\n",
    "        add_vocabs.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('voc.txt','a+',encoding = 'utf-8') as f:\n",
    "    for i in add_vocabs:\n",
    "        f.write(str(i) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('voc.txt','r',encoding = 'utf-8') as f:\n",
    "    add_vocabs = [i.strip() for i in f.readlines()]\n",
    "    \n",
    "base_tokenizer.add_tokens(add_vocabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepare_Train_Features_For_CRF:\n",
    "    def __init__(self,tokenizer,max_length = 384,stride = 128,pad_on_right = \"right\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.pad_on_right = pad_on_right\n",
    "        \n",
    "    def FindOffset(self,tokens_id, answer_id):\n",
    "        n = len(tokens_id)\n",
    "        m = len(answer_id)\n",
    "        if n < m:\n",
    "            return False\n",
    "        for i in range(n - m + 1):\n",
    "            if tokens_id[i:i + m] == answer_id:\n",
    "                return (i, i + m)\n",
    "        return False\n",
    "\n",
    "    def prepare_train_features(self,examples):\n",
    "        # 用 truncation和padding来Tokenize我们的实例，但用stride来保持溢出。\n",
    "        # 这就导致了当context较长时，一个实例可能会给出几个feature，\n",
    "        # 每个feature的context都与前一个feature的context有一点重叠。\n",
    "        tokenized_examples = self.tokenizer(\n",
    "            examples[\"question\" if self.pad_on_right else \"context\"],\n",
    "            examples[\"context\" if self.pad_on_right else \"question\"],\n",
    "            truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
    "            max_length= self.max_length,\n",
    "            stride= self.stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "        tokenized_examples[\"answer_offset\"] = []\n",
    "        tokenized_examples[\"answer_seq_label\"] = []\n",
    "        tokenized_examples[\"labels\"] = []\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]           \n",
    "            answer_seq_label = len(input_ids) * [0]\n",
    "            #print(len( answer_seq_label))\n",
    "            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
    "\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            \n",
    "           \n",
    "            answers = examples[\"answers\"][sample_index]\n",
    "            \n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"answer_offset\"].append((-1,-1))\n",
    "                tokenized_examples[\"answer_seq_label\"].append(answer_seq_label)\n",
    "                tokenized_examples[\"labels\"].append(0)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"answer_offset\"].append((-1,-1))\n",
    "                    tokenized_examples[\"answer_seq_label\"].append(answer_seq_label)\n",
    "                    tokenized_examples[\"labels\"].append(0)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                   \n",
    "                    # 否则就把token_start_index和token_end_index移到答案的两端。\n",
    "                    # 注意：如果答案是最后一个字，我们可以移到最后一个offset 之后(极少出现之情形)\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "                    \n",
    "                    answer_tokens = self.tokenizer.encode(answers[\"text\"][0])\n",
    "                    if self.FindOffset(input_ids, answer_tokens[1:-1]):\n",
    "                        #print(answer_tokens[1:-1])\n",
    "                        answer_offset = self.FindOffset(input_ids, answer_tokens[1:-1]) #有肯能返回False\n",
    "                        tokenized_examples[\"answer_offset\"].append(self.FindOffset(input_ids, answer_tokens[1:-1]))\n",
    "                        answer_seq_label[answer_offset[0]:answer_offset[1]] = [1]*(len(answer_tokens[1:-1]))\n",
    "                        tokenized_examples[\"answer_seq_label\"].append(answer_seq_label)\n",
    "                        tokenized_examples[\"labels\"].append(1)\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        tokenized_examples[\"answer_offset\"].append(answer_offset)\n",
    "                        #answer_seq_label[answer_offset[0]:answer_offset[1]] = [1]*(len(answer_tokens[1:-1]))\n",
    "                        tokenized_examples[\"answer_seq_label\"].append(answer_seq_label)\n",
    "                        tokenized_examples[\"labels\"].append(0)\n",
    "\n",
    "        return tokenized_examples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "#prepare_features = Prepare_Train_Features(base_tokenizer,max_length = 384,stride = 128,pad_on_right = \"right\")\n",
    "prepare_features = Prepare_Train_Features_For_CRF(base_tokenizer,max_length = 384,stride = 128,pad_on_right = \"right\")\n",
    "#ds = prepare_features.prepare_train_features(datasets['train'][21:22])\n",
    "#ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'省 人 社 厅 、 省 工 信 厅 、 省 教 育 厅 、 省 财 政 厅 、 省 交 通 运 输 厅 、 省 卫 健 委'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.decode([4689, 782, 4852, 1324, 510, 4689, 2339, 928, 1324, 510, 4689, 3136, 5509, 1324, 510, 4689, 6568, 3124, 1324, \n",
    "                       510, 4689, 769, 6858, 6817, 6783, 1324, 510, 4689, 1310, 978, 1999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7148e6bde98c40388933c9335438372f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782e4e1ccd9d459dbc361638c41812a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "                                  prepare_features.prepare_train_features, \n",
    "                                  batched=True,\n",
    "                                  load_from_cache_file=False,\n",
    "                                  remove_columns=datasets[\"train\"].column_names\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_offset': [171, 201], 'answer_seq_label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'end_positions': 200, 'input_ids': [101, 7151, 2190, 7028, 6206, 3696, 4495, 1555, 1501, 8024, 2356, 1767, 4664, 5052, 2600, 2229, 2347, 2972, 1914, 2157, 821, 689, 1403, 4852, 833, 976, 1139, 2582, 3416, 4638, 2824, 6437, 8043, 102, 7305, 794, 7028, 510, 794, 2571, 510, 794, 698, 2802, 1140, 1392, 5102, 817, 3419, 6824, 3791, 6121, 711, 511, 2779, 5635, 123, 3299, 128, 3189, 8024, 1059, 1744, 2356, 1767, 4664, 5052, 6956, 7305, 1066, 4989, 3428, 3868, 1350, 3696, 4495, 1555, 1501, 817, 3419, 4638, 3428, 816, 8286, 1914, 816, 8024, 2347, 5307, 1905, 5385, 8135, 1914, 816, 511, 21167, 7028, 2891, 1139, 1140, 8024, 7188, 5580, 3780, 744, 8024, 1104, 679, 6375, 6824, 3791, 6121, 711, 2501, 2768, 3698, 952, 511, 21168, 800, 6432, 511, 2945, 792, 5305, 8024, 2356, 1767, 4664, 5052, 2600, 2229, 2347, 2972, 1220, 4289, 5401, 510, 7350, 7027, 2349, 2349, 510, 3636, 3727, 704, 4636, 797, 996, 6631, 2356, 3300, 7361, 1062, 1385, 5023, 8114, 1914, 2157, 821, 689, 712, 1220, 1403, 4852, 833, 976, 1139, 2824, 6437, 8024, 1762, 4554, 2658, 7344, 2971, 3309, 7313, 7028, 6206, 3696, 4495, 1555, 1501, 21167, 817, 3419, 679, 3885, 510, 6574, 7030, 679, 7360, 510, 897, 2418, 679, 3171, 21168, 511, 2779, 3632, 1168, 4680, 1184, 711, 3632, 8024, 2347, 3300, 8269, 1914, 2157, 821, 689, 712, 1220, 1346, 680, 8024, 3868, 1350, 7305, 2421, 122, 119, 127, 674, 1914, 2157, 511, 7357, 2562, 3736, 6851, 7463, 8024, 794, 4680, 1184, 2337, 3389, 4664, 3844, 4638, 8164, 702, 1814, 2356, 3144, 2945, 3341, 4692, 8024, 2600, 860, 3341, 6432, 5101, 6150, 2094, 510, 5831, 5074, 2094, 7028, 6206, 3696, 4495, 1555, 1501, 817, 3419, 1825, 3315, 2398, 4937, 8024, 897, 2418, 1041, 6639, 8024, 671, 763, 3362, 5922, 5102, 1555, 1501, 817, 3419, 678, 7360, 3683, 6772, 3209, 3227, 511, 21167, 2769, 812, 2199, 5326, 5330, 1068, 3800, 7028, 6206, 3696, 4495, 1555, 1501, 817, 3419, 8024, 1217, 2487, 2337, 3389, 4664, 3844, 8024, 2487, 1265, 2809, 3791, 1215, 3428, 8024, 2471, 2193, 4852, 833, 1392, 4518, 1066, 1398, 5335, 2844, 7028, 6206, 3696, 4495, 1555, 1501, 817, 3419, 4914, 2415, 511, 21168, 800, 6432, 511, 3417, 7000, 3466, 3844, 6407, 1177, 1377, 7479, 1408, 8043, 21167, 1724, 702, 3297, 698, 21168, 4664, 5052, 8013, 3173, 1094, 5511, 4142, 4567, 3681, 3417, 7000, 102], 'labels': 1, 'start_positions': 171, 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][1120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查数据的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 针 对 重 要 民 生 商 品 ， 市 场 监 管 总 局 已 推 多 家 企 业 向 社 会 做 出 怎 样 的 承 诺 ？ [SEP] 门 从 重 、 从 快 、 从 严 打 击 各 类 价 格 违 法 行 为 。 截 至 2 月 7 日 ， 全 国 市 场 监 管 部 门 共 立 案 涉 及 民 生 商 品 价 格 的 案 件 360 多 件 ， 已 经 处 罚 100 多 件 。 “ 重 拳 出 击 ， 铁 腕 治 乱 ， 决 不 让 违 法 行 为 形 成 气 候 。 ” 他 说 。 据 介 绍 ， 市 场 监 管 总 局 已 推 动 物 美 、 阿 里 巴 巴 、 武 汉 中 百 仓 储 超 市 有 限 公 司 等 30 多 家 企 业 主 动 向 社 会 做 出 承 诺 ， 在 疫 情 防 控 期 间 重 要 民 生 商 品 “ 价 格 不 涨 、 质 量 不 降 、 供 应 不 断 ” 。 截 止 到 目 前 为 止 ， 已 有 150 多 家 企 业 主 动 参 与 ， 涉 及 门 店 1. 6 万 多 家 。 陈 志 江 透 露 ， 从 目 前 巡 查 监 测 的 40 个 城 市 数 据 来 看 ， 总 体 来 说 米 袋 子 、 菜 篮 子 重 要 民 生 商 品 价 格 基 本 平 稳 ， 供 应 充 足 ， 一 些 果 蔬 类 商 品 价 格 下 降 比 较 明 显 。 “ 我 们 将 继 续 关 注 重 要 民 生 商 品 价 格 ， 加 强 巡 查 监 测 ， 强 化 执 法 办 案 ， 引 导 社 会 各 界 共 同 维 护 重 要 民 生 商 品 价 格 秩 序 。 ” 他 说 。 核 酸 检 测 试 剂 可 靠 吗 ？ “ 四 个 最 严 ” 监 管 ！ 新 冠 肺 炎 病 毒 核 酸 [SEP]\n",
      "在疫情防控期间重要民生商品“价格不涨、质量不降、供应不断”。\n"
     ]
    }
   ],
   "source": [
    "ex_id = 1120\n",
    "indexs = tokenized_datasets['train'][ex_id]['answer_offset']\n",
    "print(base_tokenizer.decode(tokenized_datasets['train'][ex_id]['input_ids']))\n",
    "print(''.join(base_tokenizer.decode(tokenized_datasets['train'][ex_id]['input_ids']).split()[indexs[0]: indexs[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{384: 31708}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([len(i) for i in tokenized_datasets['train']['answer_seq_label']])\n",
    "print (dict(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{384: 31708}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([len(i) for i in tokenized_datasets['train']['input_ids']])\n",
    "print (dict(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from transformers import AutoModelForQuestionAnswering, BertPreTrainedModel,BertModel\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "from utils import CRF\n",
    "\n",
    " \n",
    "@dataclass        \n",
    "class QuestionAnsweringModelOutputWithMultiTask_CRF(QuestionAnsweringModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    categorized_logits: torch.FloatTensor = None\n",
    "    crf_logits: torch.FloatTensor = None\n",
    "    start_logits: torch.FloatTensor = None\n",
    "    end_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "        \n",
    "# Example Usage:- smooth_one_hot(torch.tensor([2, 3]), classes=10, smoothing=0.1)\n",
    "def smooth_one_hot(true_labels: torch.Tensor, classes: int, smoothing=0.0):\n",
    "    \"\"\"\n",
    "  if smoothing == 0, it's one-hot method\n",
    "  if 0 < smoothing < 1, it's smooth method\n",
    "  \"\"\"\n",
    "    assert 0 <= smoothing < 1\n",
    "    confidence = 1.0 - smoothing\n",
    "    #print(f\"Confidence:{confidence}\")\n",
    "    label_shape = torch.Size((true_labels.size(0), classes))\n",
    "    #print(f\"Label Shape:{label_shape}\")\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.empty(size=label_shape, device=true_labels.device)\n",
    "        #print(f\"True Distribution:{true_dist}\")\n",
    "        true_dist.fill_(smoothing / (classes - 1))\n",
    "        #print(f\"First modification to True Distribution:{true_dist}\")\n",
    "        true_dist.scatter_(1, true_labels.data.unsqueeze(1), confidence)\n",
    "    #print(f\"Modified Distribution:{true_dist}\")\n",
    "    return true_dist\n",
    "\n",
    "def cross_entropy(input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "  Args:\n",
    "        pred: predictions for neural network\n",
    "        targets: targets, can be soft\n",
    "        size_average: if false, sum is returned instead of mean\n",
    "  \"\"\"\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(input), dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))\n",
    "      \n",
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    \n",
    "    smooth_start_positions = smooth_one_hot(start_positions, classes=384, smoothing=0.1)\n",
    "    smooth_end_positions = smooth_one_hot(end_positions, classes=384, smoothing=0.1)\n",
    "\n",
    "    start_loss = cross_entropy(start_logits, smooth_start_positions)\n",
    "    end_loss = cross_entropy(end_logits, smooth_end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "  \n",
    "    return total_loss        \n",
    "\n",
    "@dataclass        \n",
    "class QuestionAnsweringModelOutputWithMultiTask(QuestionAnsweringModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    categorized_logits: torch.FloatTensor = None\n",
    "    start_logits: torch.FloatTensor = None\n",
    "    end_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "class BertForQuestionAnsweringWithMultiTask_CRF(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.high_dropout = nn.Dropout(p=0.5) \n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        self.bert = BertModel(config)\n",
    "        self.span_classifier = nn.Linear(config.hidden_size*2, config.num_labels, bias=True)\n",
    "        self.include_classifier = nn.Linear(config.hidden_size, config.num_labels, bias=True)\n",
    "        self.CRF_fc1 = nn.Sequential(\n",
    "            self.high_dropout,\n",
    "            nn.Linear(config.hidden_size, config.num_labels + 2, bias=True),\n",
    "            )\n",
    "        \n",
    "        self.CRF = CRF(target_size = self.bert.config.num_labels,device= torch.device(\"cuda\"))\n",
    "        self.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "        self.fc2 = nn.Linear(config.hidden_size, 2, bias=True)\n",
    "        \n",
    "        assert config.num_labels == 2\n",
    "        \n",
    "        torch.nn.init.normal_(self.span_classifier.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.include_classifier.weight, std=0.02)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids= None,\n",
    "        answer_offset= None,\n",
    "        answer_seq_label= None,\n",
    "        labels=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "       \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states = True,\n",
    "            return_dict= True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        last_hidden_state_output = bert_output.last_hidden_state[:,0,:]  # (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_state_output = self.dropout(last_hidden_state_output)\n",
    "        include_logits = self.include_classifier(last_hidden_state_output)\n",
    "        \n",
    "        #################################### CRF #############################################################################\n",
    "        \n",
    "        batch_size, seq_length = input_ids[:,1:].size() # 计算sql_len 不包含[CLS]\n",
    " \n",
    "        # CRF mask\n",
    "        mask = np.ones(shape=[batch_size, seq_length], dtype=np.uint8)\n",
    "        mask = torch.ByteTensor(mask).bool().to('cuda')  # [batch_size, seq_len, 4]\n",
    "        #print('mask',mask.shape )\n",
    "\n",
    "        # No [CLS]\n",
    "        #print(bert_output.last_hidden_state[:,1:,:].shape)\n",
    "        crf_logits = self.CRF_fc1(bert_output.last_hidden_state[:,1:,:] )\n",
    "        #_, CRFprediction = self.CRF.forward(feats=crf_logits, mask=mask)   \n",
    "        \n",
    "        #################################### Span #############################################################################\n",
    "        span_hidden_states = bert_output.hidden_states # (batch_size, sequence_length, hidden_size)\n",
    "        span_out = torch.stack((span_hidden_states[-1], span_hidden_states[-2], span_hidden_states[-3], span_hidden_states[-4]), dim=0)  #最后四层拼接\n",
    "        span_out_mean = torch.mean(span_out, dim=0)\n",
    "        span_out_max, _ = torch.max(span_out, dim=0)\n",
    "        span_out = torch.cat((span_out_mean, span_out_max), dim=-1)\n",
    "        span_logits = torch.mean(torch.stack([ self.span_classifier(self.high_dropout(span_out))for _ in range(5) ], dim=0), dim=0)\n",
    "        #######################################################################################################################\n",
    "        start_logits, end_logits = span_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)  # (bs, max_query_len)\n",
    "        end_logits = end_logits.squeeze(-1)  # (bs, max_query_len)\n",
    "    \n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None and labels is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            #sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "            span_loss = loss_fn(start_logits, end_logits, start_positions, end_positions)\n",
    "            include_loss = nn.CrossEntropyLoss()(include_logits, labels)\n",
    "            crf_loss = self.CRF.neg_log_likelihood_loss(feats=crf_logits, mask=mask, tags=answer_seq_label[:,1:] )\n",
    "            total_loss = span_loss + include_loss + crf_loss\n",
    "            \n",
    "        return QuestionAnsweringModelOutputWithMultiTask_CRF(\n",
    "                        loss= total_loss,\n",
    "                        start_logits=start_logits,\n",
    "                        end_logits=end_logits,\n",
    "                        categorized_logits = include_logits  ,\n",
    "                        crf_logits=crf_logits,\n",
    "                        hidden_states= bert_output.hidden_states,\n",
    "                        attentions= bert_output.attentions  )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at chinese-bert-wwm-ext were not used when initializing BertForQuestionAnsweringWithMultiTask_CRF: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnsweringWithMultiTask_CRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnsweringWithMultiTask_CRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnsweringWithMultiTask_CRF were not initialized from the model checkpoint at chinese-bert-wwm-ext and are newly initialized: ['span_classifier.weight', 'span_classifier.bias', 'include_classifier.weight', 'include_classifier.bias', 'CRF_fc1.1.weight', 'CRF_fc1.1.bias', 'CRF.transitions', 'fc2.weight', 'fc2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "mode_path =  r'chinese-bert-wwm-ext'\n",
    "\n",
    "model = BertForQuestionAnsweringWithMultiTask_CRF.from_pretrained(mode_path)\n",
    "model.resize_token_embeddings(len(base_tokenizer)) \n",
    "question, text = \"毛泽东是谁？\", \"毛泽东是国家主席。\"\n",
    "inputs = base_tokenizer(question, text, return_tensors='pt')\n",
    "start_positions = torch.tensor([4])\n",
    "end_positions = torch.tensor([7])\n",
    "labels =  torch.tensor([1])\n",
    "outputs = model(**inputs, \n",
    "                #start_positions=start_positions, \n",
    "                #end_positions=end_positions,\n",
    "                #labels =labels\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 毛 泽 东 是 谁 ？ [SEP] 毛 泽 东 是 国 家 “ 主 席 ” 。 [SEP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.decode(base_tokenizer.encode(\"毛泽东是谁？\", \"毛泽东是国家“主席”。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(16), tensor(11)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.argmax(outputs.start_logits),torch.argmax(outputs.end_logits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test-squad-trained\\\\tokenizer_config.json',\n",
       " 'test-squad-trained\\\\special_tokens_map.json',\n",
       " 'test-squad-trained\\\\vocab.txt',\n",
       " 'test-squad-trained\\\\added_tokens.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.save_pretrained(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-squad-trained\",\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    #evaluation_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps =2000,\n",
    "    save_steps = 1500,\n",
    "    eval_steps = 2000,\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit = 10,\n",
    "    weight_decay=0.01,\n",
    "    label_names = [\"start_positions\", \"end_positions\"] ,\n",
    "    seed = 2021\n",
    "            \n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator= default_data_collator,\n",
    "    tokenizer=base_tokenizer,\n",
    "   \n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-squad-trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: gaochangkuan (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.5<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">test-squad-trained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gaochangkuan/huggingface\" target=\"_blank\">https://wandb.ai/gaochangkuan/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gaochangkuan/huggingface/runs/3lyxu94r\" target=\"_blank\">https://wandb.ai/gaochangkuan/huggingface/runs/3lyxu94r</a><br/>\n",
       "                Run data is saved locally in <code>wandb\\run-20210224_095948-3lyxu94r</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2001' max='7927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2001/7927 38:29 < 1:54:05, 0.87 it/s, Epoch 0.25/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='81' max='7927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  81/7927 00:37 < 1:01:49, 2.11 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda20190415\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_epoch_stop\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda20190415\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda20190415\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   1328\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m         )\n\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda20190415\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mprediction_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda20190415\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   1546\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1548\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1549\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-ffcfa97bcae7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, answer_offset, answer_seq_label, labels, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# CRF mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [batch_size, seq_len, 4]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;31m#print('mask',mask.shape )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test-squad-trained\\\\tokenizer_config.json',\n",
       " 'test-squad-trained\\\\special_tokens_map.json',\n",
       " 'test-squad-trained\\\\vocab.txt',\n",
       " 'test-squad-trained\\\\added_tokens.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"test-squad-trained\")\n",
    "base_tokenizer.save_pretrained(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = trainer.eval_dataset[:40]\n",
    "batch = {k: torch.tensor(v).to(trainer.args.device) for k, v in ds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits', 'hidden_states', 'categorized_logits', 'crf_logits'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best_size = 10\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And then we can sort the valid_answers according to their score and only keep the best one. The only point left is how to check a given span is \n",
    "# inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
    "#the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
    "#the offset mapping that will give us a map from token indices to character positions in the context.\n",
    "#That's why we will re-process the validation set with the following function, slightly different from prepare_train_features:\n",
    "\n",
    "# 然后我们可以根据得分对valid_answers 进行排序，只保留最好的一个。唯一剩下的问题是如何检查一个给定的span 是否在context中（而不是在question中），\n",
    "#以及如何取回里面的文本。要做到这一点，我们需要在我们的validation features中添加两样东西：\n",
    "# 1）生成特征的例子的ID（因为每个实例可以生成多个feature，如前所述）；\n",
    "# 2）offset mapping ，将给我们一个从标记索引到context中字符位置的映射。\n",
    "# 这就是为什么我们要用下面的函数 re-process验证集，与prepare_train_features略有不同的原因。\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = base_tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513235f5c55b40ad9d26288417187347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pad_on_right = 'right'\n",
    "\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'example_id': 'edd1413c78e534afb136f36fdc9c9a00', 'input_ids': [101, 4886, 2456, 5468, 1394, 1139, 1378, 3265, 821, 2974, 3177, 3118, 2898, 1908, 2339, 4937, 2266, 4638, 6956, 7305, 6963, 3300, 6443, 8043, 102, 1227, 1218, 6783, 1139, 4689, 819, 5041, 6370, 1227, 1218, 1291, 868, 1291, 6379, 510, 6392, 4989, 1227, 1218, 1291, 868, 2339, 868, 4991, 8024, 2190, 3680, 702, 2339, 868, 4991, 5314, 750, 671, 3613, 2595, 8108, 674, 1039, 2218, 689, 3302, 1218, 5307, 6589, 6133, 1221, 511, 7961, 1225, 831, 1044, 5470, 4500, 3315, 1765, 1227, 1218, 782, 1447, 511, 3313, 5307, 4689, 2418, 2190, 3173, 1094, 5511, 4142, 4554, 2658, 2339, 868, 3300, 1068, 3322, 3354, 4802, 6371, 4638, 4554, 2658, 7344, 2971, 2593, 7444, 4289, 6598, 4495, 772, 821, 689, 2471, 6822, 1227, 1220, 1213, 4638, 8024, 671, 3613, 2595, 4500, 2339, 3302, 1218, 1946, 6133, 3403, 1114, 3297, 7770, 2990, 1168, 3680, 782, 8202, 1039, 511, 2190, 677, 6835, 821, 689, 1780, 2898, 1762, 4495, 772, 671, 5296, 2339, 868, 4638, 5466, 2339, 8024, 5314, 750, 3680, 782, 3680, 1921, 8135, 1039, 4638, 4495, 3833, 6133, 1221, 8024, 5287, 1057, 671, 3613, 2595, 4500, 2339, 3302, 1218, 1946, 6133, 5745, 4533, 511, 2190, 3217, 5688, 2496, 3299, 5635, 4554, 2658, 671, 5277, 1510, 2418, 5310, 3338, 3299, 8024, 7023, 1357, 4937, 2137, 5466, 2339, 7339, 824, 924, 2898, 6825, 5330, 4495, 772, 4638, 821, 689, 8024, 5314, 750, 671, 3613, 2595, 4937, 2218, 689, 1946, 6133, 511, 1217, 1920, 1927, 689, 924, 7372, 4937, 2266, 6819, 6820, 1213, 2428, 8024, 2199, 704, 2207, 2544, 821, 689, 4937, 2266, 6819, 6820, 3124, 5032, 6161, 1447, 4372, 3403, 1114, 6444, 3146, 711, 679, 7770, 754, 677, 2399, 2428, 1059, 1744, 6444, 3389, 1927, 689, 4372, 4638, 2971, 1169, 4680, 3403, 8024, 2190, 1346, 924, 5466, 2339, 8114, 782, 8020, 1419, 8021, 809, 678, 4638, 821, 689, 8024, 6161, 1447, 4372, 6444, 3146, 711, 679, 6631, 6814, 821, 689, 1346, 924, 5466, 2339, 2600, 3144, 4638, 8113, 110, 511, 2190, 679, 6161, 1447, 2772, 2208, 6161, 1447, 8024, 5016, 1394, 3340, 816, 4638, 1346, 924, 821, 689, 8024, 1377, 6819, 6820, 1071, 677, 2399, 2428, 2141, 7354, 5373, 5287, 1927, 689, 924, 7372, 6589, 4638, 8145, 110, 511, 2190, 1358, 4554, 2658, 2512, 1510, 7481, 707, 3257, 3198, 2595, 4495, 772, 5307, 5852, 1737, 7410, 684, 2612, 102], 'offset_mapping': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, [234, 235], [235, 236], [236, 237], [237, 238], [238, 239], [239, 240], [240, 241], [241, 242], [242, 243], [243, 244], [244, 245], [245, 246], [246, 247], [247, 248], [248, 249], [249, 250], [250, 251], [251, 252], [252, 253], [253, 254], [254, 255], [255, 256], [256, 257], [257, 258], [258, 259], [259, 260], [260, 261], [261, 262], [262, 263], [263, 264], [264, 265], [265, 266], [266, 267], [267, 268], [268, 269], [269, 270], [270, 272], [272, 273], [273, 274], [274, 275], [275, 276], [276, 277], [277, 278], [278, 279], [279, 280], [280, 281], [281, 282], [282, 283], [283, 284], [284, 285], [285, 286], [286, 287], [287, 288], [288, 289], [289, 290], [290, 291], [291, 292], [292, 293], [293, 294], [294, 295], [295, 296], [297, 298], [298, 299], [299, 300], [300, 301], [301, 302], [302, 303], [303, 304], [304, 305], [305, 306], [306, 307], [307, 308], [308, 309], [309, 310], [310, 311], [311, 312], [312, 313], [313, 314], [314, 315], [315, 316], [316, 317], [317, 318], [318, 319], [319, 320], [320, 321], [321, 322], [322, 323], [323, 324], [324, 325], [325, 326], [326, 327], [327, 328], [328, 329], [329, 330], [330, 331], [331, 332], [332, 333], [333, 334], [334, 335], [335, 336], [336, 337], [337, 338], [338, 339], [339, 340], [340, 341], [341, 342], [342, 343], [343, 344], [344, 345], [345, 346], [346, 347], [347, 348], [348, 349], [349, 350], [350, 351], [351, 352], [352, 353], [353, 357], [357, 358], [358, 359], [359, 360], [360, 361], [361, 362], [362, 363], [363, 364], [364, 365], [365, 366], [366, 367], [367, 368], [368, 369], [369, 370], [370, 371], [371, 372], [372, 373], [373, 374], [374, 375], [375, 376], [376, 377], [377, 378], [378, 379], [379, 380], [380, 381], [381, 382], [382, 383], [383, 386], [386, 387], [387, 388], [388, 389], [389, 390], [390, 391], [391, 392], [392, 393], [393, 394], [394, 395], [395, 396], [396, 397], [397, 398], [398, 399], [399, 400], [400, 401], [401, 402], [402, 403], [403, 404], [404, 405], [405, 406], [406, 407], [407, 408], [408, 409], [409, 410], [410, 411], [411, 412], [412, 413], [413, 414], [414, 415], [415, 416], [416, 417], [417, 418], [418, 419], [419, 420], [420, 421], [421, 422], [422, 423], [423, 424], [424, 425], [425, 426], [426, 427], [427, 428], [428, 429], [429, 430], [430, 431], [431, 432], [432, 433], [433, 434], [434, 435], [435, 436], [436, 437], [437, 438], [438, 439], [439, 440], [440, 441], [441, 442], [442, 443], [443, 444], [444, 445], [445, 446], [446, 447], [447, 448], [448, 449], [449, 450], [450, 451], [451, 452], [453, 454], [454, 455], [455, 456], [456, 457], [457, 458], [458, 459], [459, 460], [460, 461], [461, 462], [462, 463], [463, 464], [464, 465], [465, 466], [466, 467], [467, 468], [468, 469], [469, 470], [470, 471], [471, 472], [472, 473], [473, 474], [474, 475], [475, 476], [476, 477], [477, 478], [478, 479], [479, 480], [480, 481], [481, 482], [482, 483], [483, 484], [484, 485], [485, 486], [486, 487], [487, 488], [488, 489], [489, 490], [490, 491], [491, 492], [492, 493], [493, 494], [494, 495], [495, 496], [496, 497], [497, 498], [498, 499], [499, 500], [500, 501], [501, 502], [502, 503], [503, 504], [504, 505], [505, 506], [506, 507], [507, 508], [508, 509], [509, 510], [510, 512], [512, 513], [513, 514], [514, 515], [515, 516], [516, 517], [517, 518], [518, 519], [519, 520], [520, 521], [521, 522], [522, 523], [523, 524], [524, 525], [525, 526], [526, 527], [527, 528], [528, 529], [529, 530], [530, 531], [531, 532], [532, 533], [533, 534], [534, 535], [535, 536], [536, 537], [537, 538], [538, 539], [539, 540], [540, 542], [542, 543], [543, 544], [544, 545], [545, 546], [546, 547], [547, 548], [548, 549], [549, 550], [550, 551], [551, 552], [552, 553], [553, 554], [554, 555], [555, 556], [556, 557], [557, 558], [558, 559], [559, 560], [560, 561], [561, 562], [562, 563], [563, 564], [564, 565], [565, 566], [566, 567], [567, 568], [568, 569], [569, 570], [570, 571], [571, 572], [572, 573], [573, 574], [574, 575], [575, 576], [576, 577], [577, 578], [578, 579], [579, 580], [580, 582], [582, 583], [583, 584], [584, 585], [585, 586], [586, 587], [587, 588], [588, 589], [589, 590], [590, 591], [591, 592], [592, 593], [593, 594], [594, 595], [595, 596], [596, 597], [597, 598], [598, 599], [599, 600], [600, 601], [601, 602], [602, 603], None], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(validation_features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = trainer.predict(validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: 北京市网上办理登记注册事项的服务平台是什么？ context: 北京市市场监督管理局关于在疫情防控期间提倡网上办理登记注册事项的倡议书 为全面加强疫情防控，北京市市场监督管理局向您发出倡议:“网上办理业务，有效减少聚集，强化疫情防控”。 请登录北京市企业登记“e窗通”服务平台(网址：etc.scjgj.beijing.gov.cn)全程网上办理相关业务，我们将为你提供便捷高效的服务。 需现场提交材料的业务，建议您延期办理。您可选择通过在线咨询（各登记注册部门咨询电话详见下表）、网上预审的方式先行沟通。登记注册部门已安排专业人员进行业务办理咨询和指导。 确需现场办理的事项，请您自觉正确佩戴口罩，业务办理完毕後尽快离开。伴有发热、咳嗽等不适症状者，建议暂不到现场办理业务。 在此期间，给您带来的不便敬请谅解，衷心感谢您的理解和支持！业务咨询、投诉建议请致电010-11616611。相关办事指南、政策宣传敬请关注“北京市市场监督管理局”官网。                        北京市市场监督管理局                          二○二○年二月三日 各登记注册部门咨询电话 answer: “e窗通”服务平台(网址：etc.scjgj.beijing.gov.cn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': -6.7814484, 'text': '办理相关业'},\n",
       " {'score': -7.08699, 'text': '办理相关'},\n",
       " {'score': -7.089334, 'text': 'gov.cn)全程网上办理相关业'},\n",
       " {'score': -7.3057723, 'text': '业'},\n",
       " {'score': -7.3867755, 'text': '办理'},\n",
       " {'score': -7.3948755, 'text': 'gov.cn)全程网上办理相关'},\n",
       " {'score': -7.4955273, 'text': 'gov.'},\n",
       " {'score': -7.5838537, 'text': '已安排专'},\n",
       " {'score': -7.603981, 'text': '.cn)全程网上办理相关业'},\n",
       " {'score': -7.6684933, 'text': 'v.cn)全程网上办理相关业'},\n",
       " {'score': -7.670457, 'text': 'gov'},\n",
       " {'score': -7.6859384, 'text': '办理相关业务'},\n",
       " {'score': -7.694661, 'text': 'gov.cn)全程网上办理'},\n",
       " {'score': -7.7009935, 'text': '相关业'},\n",
       " {'score': -7.727461, 'text': '已安'},\n",
       " {'score': -7.739602, 'text': '关业'},\n",
       " {'score': -7.8180265, 'text': '已安排'},\n",
       " {'score': -7.832142, 'text': '发热、咳嗽等'},\n",
       " {'score': -7.8447456, 'text': '有发热、咳嗽等'},\n",
       " {'score': -7.846612, 'text': '等'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_index = 25\n",
    "max_answer_length = 50\n",
    "n_best_size = 20\n",
    "\n",
    "start_logits = output.start_logits[ex_index].cpu().numpy()\n",
    "end_logits = output.end_logits[ex_index].cpu().numpy()\n",
    "offset_mapping = validation_features[ex_index][\"offset_mapping\"]\n",
    "\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "\n",
    "context = datasets[\"validation\"][ex_index][\"context\"]\n",
    "print('question:', datasets[\"validation\"][ex_index][\"question\"],\n",
    "      'context:',context,\n",
    "      'answer:',datasets[\"validation\"][ex_index][\"answers\"]['text'][0])\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(base_tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score > feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if squad_v2:\n",
    "    import os\n",
    "    # Adapt this to your local environment\n",
    "    path_to_transformers = \"../git/transformers\"\n",
    "    path_to_qa_examples = os.path.join(path_to_transformers, \"examples/question-answering\")\n",
    "    metric = load_metric(os.path.join(path_to_qa_examples, \"squad_v2_local\"))\n",
    "    # Uncomment when the fix is merged in master and has been released.\n",
    "    # metric = load_metric(\"squad_v2\")\n",
    "else:\n",
    "    metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def answerQuestion(question, paper):\n",
    "    \"\"\"\n",
    "    This funtion provides the best answer found by the Q&A model, the chunk containing it\n",
    "    among all chunks of the input paper and the score obtained by the answer.\n",
    "    该方法提供了问答模型找到的最佳答案，在所有输入chunk中包含该答案的chunks，以及由该答案得到的分数（置信度）。\n",
    "    \"\"\"\n",
    "    paper = [paragraph for paragraph in paper if len(paragraph)>0]\n",
    "    inputs = [base_tokenizer.encode_plus(\n",
    "        question, paragraph.replace('\\n','').replace('\\t','').replace(' ',''), \n",
    "               add_special_tokens=True, return_tensors=\"pt\") for paragraph in paper ]\n",
    "    answers = []\n",
    "    confidence_scores = []\n",
    "    for n, Input in enumerate(inputs):\n",
    "        input_ids = Input['input_ids'].to(torch_device)\n",
    "        token_type_ids = Input['token_type_ids'].to(torch_device)\n",
    "        attention_masks = Input['attention_mask'].to(torch_device)\n",
    "        if len(input_ids[0]) > 512:\n",
    "            input_ids = input_ids[:, :512]\n",
    "            token_type_ids = token_type_ids[:, :512]\n",
    "            attention_masks = attention_masks[:, :512]\n",
    "            \n",
    "        text_tokens = base_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        outputs = model(    input_ids,\n",
    "                            token_type_ids =token_type_ids,\n",
    "                            attention_mask = attention_masks  \n",
    "                                              )\n",
    "        \n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        \n",
    "        # 如果答案的起始标记包含在问题中，起始标记就会被移动到该chunk的第一个。\n",
    "        check = text_tokens.index(\"[SEP]\")\n",
    "        if int(answer_start) <= check:\n",
    "            answer_start = check+1\n",
    "        answer = base_tokenizer.convert_tokens_to_string(text_tokens[answer_start:(answer_end+1)])\n",
    "        answer = answer.replace('[SEP]', '')\n",
    "        confidence = start_scores[0][answer_start] + end_scores[0][answer_end]\n",
    "        if answer.startswith('。') or answer.startswith('，'):\n",
    "            answer = answer[2:]\n",
    "        answers.append(answer)\n",
    "        confidence_scores.append(float(confidence))\n",
    "    \n",
    "    maxIdx = np.argmax(confidence_scores)\n",
    "    confidence = confidence_scores[maxIdx]\n",
    "    best_answer = answers[maxIdx]\n",
    "    best_paragraph = paper[maxIdx]\n",
    "\n",
    "    return best_answer.replace(' ',''), confidence, best_paragraph.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkAnyStop(token_list, token_stops):\n",
    "    return any([stop in token_list for stop in token_stops])\n",
    "\n",
    "def firstFullStopIdx(token_list, token_stops):\n",
    "    \"\"\"\n",
    "    Returns the index of first full-stop token appearing.  \n",
    "    \"\"\"\n",
    "    idxs = []\n",
    "    for stop in token_stops:\n",
    "        if stop in token_list:\n",
    "            idxs.append(token_list.index(stop))\n",
    "    minIdx = min(idxs) if idxs else None\n",
    "    return minIdx\n",
    "\n",
    "\n",
    "puncts = ['？', '。', '?', '；',\"！\",\"!\",';']\n",
    "puncts_tokens = [base_tokenizer.tokenize(x)[0] for x in puncts]\n",
    "\n",
    "def splitTokens(tokens, punct_tokens, split_length):\n",
    "    \"\"\"\n",
    "    To avoid splitting a sentence and lose the semantic meaning of it, a paper is splitted \n",
    "    into chunks in such a way that each chunk ends with a full-stop token (['？', '。', '?', '；']) \n",
    "    \"\"\"\n",
    "    splitted_tokens = []\n",
    "    while len(tokens) > 0:\n",
    "        if len(tokens) < split_length or not checkAnyStop(tokens, punct_tokens):\n",
    "            splitted_tokens.append(tokens)\n",
    "            break\n",
    "        # to not have too long parapraphs, the nearest fullstop is searched both in the previous \n",
    "        # and the next strings.\n",
    "        prev_stop_idx = firstFullStopIdx(tokens[:split_length][::-1], puncts_tokens)\n",
    "        next_stop_idx = firstFullStopIdx(tokens[split_length:], puncts_tokens)\n",
    "        if pd.isna(next_stop_idx):\n",
    "            splitted_tokens.append(tokens[:split_length - prev_stop_idx])\n",
    "            tokens = tokens[split_length - prev_stop_idx:]\n",
    "        elif pd.isna(prev_stop_idx):\n",
    "            splitted_tokens.append(tokens[:split_length + next_stop_idx + 1])\n",
    "            tokens = tokens[split_length + next_stop_idx + 1:] \n",
    "        elif prev_stop_idx < next_stop_idx:\n",
    "            splitted_tokens.append(tokens[:split_length - prev_stop_idx])\n",
    "            tokens = tokens[split_length - prev_stop_idx:]\n",
    "        else:\n",
    "            splitted_tokens.append(tokens[:split_length + next_stop_idx + 1])\n",
    "            tokens = tokens[split_length + next_stop_idx + 1:] \n",
    "    return splitted_tokens\n",
    "\n",
    "def splitParagraph(text, split_length=128):\n",
    "    text = text.replace('\\n','').replace('\\xa0','').replace('\\t','')\n",
    "    tokens = base_tokenizer.tokenize(text)\n",
    "    splitted_tokens = splitTokens(tokens, puncts_tokens, split_length)\n",
    "    return [''.join(base_tokenizer.convert_tokens_to_string(x)).replace(' ','') for x in splitted_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from gne import GeneralNewsExtractor\n",
    "\n",
    "url ='https://baike.baidu.com/item/%E6%AF%9B%E6%B3%BD%E4%B8%9C/113835?fromtitle=%E6%AF%9B%E4%B8%BB%E5%B8%AD&fromid=380922'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3823.400 QQBrowser/10.7.4307.400'}\n",
    "resp = requests.get(url ,headers=headers).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = GeneralNewsExtractor()\n",
    "result = extractor.extract(resp, noise_node_list=['//div[@class=\"comment-list\"]'])\n",
    "print(result['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda'\n",
    "model = BertForQuestionAnsweringWithMultiTask.from_pretrained(r'test-squad-trained')\n",
    "model.to(torch_device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = '''毛泽东的籍贯是哪里？'''\n",
    "question  = '''毛泽东在经济建设方面的思想是什么？'''\n",
    "question  = '''毛泽东的主要军事思想是什么？'''\n",
    "#question  = '''整风运动是什么时候开展的？'''\n",
    "#question  = '''《双十协定》是在哪里签署的？'''\n",
    "#question  = '''《双十协定》是什么时候签署的？'''\n",
    "#question  = '''毛泽东是什么时候出生的？'''\n",
    "#question = '''六所私塾读书是什么时间段？'''\n",
    "#question = '''毛泽东是什么时候死的？'''\n",
    "#question = '''毛泽东的'三个世界'的论断是什么时候提出的？'''\n",
    "#question = '''《国民政府与中共代表会谈纪要》是什么时候签署的？'''\n",
    "#question = '''枪杆子里面出政权是哪里提出的？'''\n",
    "#question = '''秋收起义是什么时候爆发的？'''\n",
    "#question = '''文化大革命是怎样爆发的？'''\n",
    "#question = '''整风运动是在什么时候发生的？'''\n",
    "#question = '''毛泽东因为什么事很惊讶？'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = splitParagraph(result['content'], split_length=128)\n",
    "answerQuestion(question, context )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text =  [\n",
    "    \n",
    "'''1月22日，华晨宇承认：我们有一个孩子。随后，张碧晨发长文承认，她于2018年秋天怀孕，当时选择了独自离开，“所以在花花完全不知情的情况下，我独自完成了孕育和生产，成功升级成一个妈妈。”''',\n",
    "'''谈到不公开的原因，华晨宇称主要事情有些复杂，怕说不清楚的话会让歌迷们担心，同时也可以让孩子在安静的环境里成长，而不被外界关注。现在既然被曝光出来了，那我们都会坦然面对大家的疑惑。这件事情可能会让歌迷们感觉到很突然，我只能希望大家理解，谢谢大家。''',\n",
    "'''华晨宇长文中谈到孩子带来对她的影响，他表示“这个孩子的到来真的治愈了我很多，我很开心上天给我带来了这样一份特别的礼物，虽然很突然，但是也很开心，我们会给孩子带来健康快乐的成长环境。最惊喜的是，她很喜欢音乐，会经常自己一个人拿着麦克风边跳边唱《斗牛》，这是她最爱的一首歌，连睡觉都要听着这首歌入睡。”''',\n",
    "'''华晨宇谈到与女儿相处细节，表示她很会撒娇，“想吃零食的时候总是用各种方式哄你开心，同样也会关心人，每次自己拿到好吃的食物的时候，总是会先说，‘爸爸妈妈吃’。她真的很可爱，也真的成长的很好，看见她我就觉得很幸福。”''',\n",
    "'''2018年秋，当我知道自己怀孕的时候，我整个人都懵了。我和花花虽然在一起，我们也憧憬过未来的生活，但计划里从没有过生孩子结婚，至少几年之内没有，所以我当时完全慌了，不知道该怎么做才是对的。可能对我而言，30岁之前生一个自己和自己爱的人的孩子，是除了唱歌做歌手以外最大的梦想。但当我做好了要生下这个孩子的决定的时候，我混乱到完全不知道怎么跟花花说，也没去想他会怎么回应我，我顾自选择了离开，选择不告诉他不让他知道，自己去完成这一切。''',\n",
    "'''我离开了他，走的时候没有说任何理由，只说了以后别联系了。很长一段时间，我不接他的电话，不回他的微信，让他找不到我，慢慢的我们就断了联系。我知道我这么做其实很愚蠢，但我实在太慌乱太害怕了，当时这个事情远远超出了我世界里的所有认知。所以在花花完全不知情的情况下，我独自完成了孕育和生产，成功升级成一个妈妈。''' ,\n",
    "'''所以，虽然我们分开这么久了，我们的生活在分开期间也发生了很大的改变，但我们努力去重新磨合，最重要的是让孩子感受到爱，感受到家庭的温暖。''',\n",
    "'''孩子健康聪明，每天都有无数的爱围绕她，她的爷爷奶奶、姥姥姥爷、爸爸妈妈都非常地爱她，她真的成长地很好。''',\n",
    "'''很抱歉这件事隐瞒了这么久，一切的隐瞒更多的是为了保护这个孩子，想让她在平静快乐的环境里茁壮长大，给她充满爱的生活。对歌迷们和所有关心我们的人说抱歉，也感谢你们看完我的文字。'''\n",
    "                \n",
    "                \n",
    "               \n",
    "               ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question = '华晨宇担心什么？'\n",
    "question = '张碧晨抱歉什么？'\n",
    "question = '张碧晨因为什么而懵逼？'\n",
    "#question = '华晨宇为什么狂喜？'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerQuestion(question, answer_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
