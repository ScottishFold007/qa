{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 134: expected 2 fields, saw 3\\nSkipping line 2130: expected 2 fields, saw 3\\nSkipping line 2877: expected 2 fields, saw 3\\nSkipping line 2955: expected 2 fields, saw 3\\nSkipping line 2974: expected 2 fields, saw 3\\nSkipping line 3038: expected 2 fields, saw 3\\nSkipping line 3052: expected 2 fields, saw 3\\nSkipping line 3053: expected 2 fields, saw 3\\nSkipping line 3080: expected 2 fields, saw 3\\nSkipping line 3086: expected 2 fields, saw 3\\nSkipping line 3094: expected 2 fields, saw 3\\nSkipping line 3115: expected 2 fields, saw 22\\nSkipping line 3180: expected 2 fields, saw 3\\nSkipping line 3189: expected 2 fields, saw 3\\nSkipping line 3191: expected 2 fields, saw 3\\nSkipping line 3197: expected 2 fields, saw 3\\nSkipping line 3203: expected 2 fields, saw 3\\nSkipping line 3209: expected 2 fields, saw 3\\nSkipping line 3215: expected 2 fields, saw 3\\nSkipping line 3611: expected 2 fields, saw 3\\nSkipping line 3613: expected 2 fields, saw 3\\nSkipping line 3716: expected 2 fields, saw 3\\nSkipping line 3729: expected 2 fields, saw 8\\nSkipping line 3802: expected 2 fields, saw 3\\nSkipping line 3864: expected 2 fields, saw 3\\nSkipping line 3896: expected 2 fields, saw 3\\nSkipping line 3932: expected 2 fields, saw 3\\nSkipping line 3984: expected 2 fields, saw 3\\nSkipping line 4001: expected 2 fields, saw 3\\nSkipping line 4012: expected 2 fields, saw 3\\nSkipping line 4054: expected 2 fields, saw 3\\nSkipping line 4118: expected 2 fields, saw 3\\nSkipping line 4119: expected 2 fields, saw 3\\nSkipping line 4124: expected 2 fields, saw 3\\nSkipping line 4171: expected 2 fields, saw 3\\nSkipping line 4188: expected 2 fields, saw 3\\nSkipping line 4189: expected 2 fields, saw 3\\nSkipping line 4192: expected 2 fields, saw 3\\nSkipping line 4260: expected 2 fields, saw 3\\nSkipping line 4265: expected 2 fields, saw 3\\nSkipping line 4281: expected 2 fields, saw 3\\nSkipping line 4282: expected 2 fields, saw 3\\nSkipping line 4285: expected 2 fields, saw 3\\nSkipping line 4339: expected 2 fields, saw 16\\nSkipping line 4354: expected 2 fields, saw 3\\nSkipping line 4377: expected 2 fields, saw 3\\nSkipping line 4490: expected 2 fields, saw 3\\nSkipping line 4495: expected 2 fields, saw 3\\nSkipping line 4522: expected 2 fields, saw 3\\nSkipping line 4531: expected 2 fields, saw 3\\nSkipping line 4575: expected 2 fields, saw 3\\nSkipping line 4712: expected 2 fields, saw 3\\nSkipping line 4727: expected 2 fields, saw 3\\nSkipping line 4732: expected 2 fields, saw 3\\nSkipping line 4743: expected 2 fields, saw 3\\nSkipping line 4747: expected 2 fields, saw 3\\nSkipping line 4771: expected 2 fields, saw 3\\nSkipping line 4773: expected 2 fields, saw 3\\nSkipping line 4788: expected 2 fields, saw 3\\nSkipping line 4812: expected 2 fields, saw 3\\nSkipping line 4821: expected 2 fields, saw 3\\nSkipping line 4875: expected 2 fields, saw 3\\nSkipping line 4897: expected 2 fields, saw 3\\nSkipping line 4930: expected 2 fields, saw 3\\nSkipping line 4963: expected 2 fields, saw 3\\nSkipping line 4970: expected 2 fields, saw 3\\nSkipping line 5224: expected 2 fields, saw 3\\nSkipping line 5315: expected 2 fields, saw 3\\nSkipping line 5620: expected 2 fields, saw 3\\nSkipping line 5623: expected 2 fields, saw 3\\nSkipping line 5626: expected 2 fields, saw 3\\nSkipping line 5628: expected 2 fields, saw 3\\nSkipping line 5688: expected 2 fields, saw 3\\nSkipping line 5689: expected 2 fields, saw 3\\nSkipping line 5690: expected 2 fields, saw 3\\nSkipping line 5692: expected 2 fields, saw 3\\nSkipping line 5729: expected 2 fields, saw 3\\nSkipping line 6068: expected 2 fields, saw 3\\nSkipping line 6369: expected 2 fields, saw 3\\nSkipping line 6396: expected 2 fields, saw 3\\nSkipping line 6439: expected 2 fields, saw 3\\nSkipping line 6514: expected 2 fields, saw 3\\nSkipping line 7293: expected 2 fields, saw 5\\nSkipping line 7574: expected 2 fields, saw 8\\nSkipping line 7670: expected 2 fields, saw 3\\nSkipping line 7705: expected 2 fields, saw 3\\nSkipping line 8282: expected 2 fields, saw 66\\nSkipping line 8476: expected 2 fields, saw 111\\n'\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(r'疫情政务问答助手数据集\\NCPPolicies_train_20200301.csv',sep = '\\t',encoding = 'utf-8')\n",
    "\n",
    "context_df = pd.read_csv(r'疫情政务问答助手数据集\\NCPPolicies_context_20200301.csv',sep = '\\t',error_bad_lines=False,encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47a41a03966431739257ef215cdc1caa</td>\n",
       "      <td>015758c216923f89991ca61c67b29f70</td>\n",
       "      <td>工业和信息化部到哪家企业进行督导检查？</td>\n",
       "      <td>北京北铃专用汽车有限公司</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17f3894afaba3f24a3ccd550fe442177</td>\n",
       "      <td>356fb8b804d73c3bab18472c36e67a3f</td>\n",
       "      <td>北京市对于因疫情影响暂停举办的展会项目有何支持政策？</td>\n",
       "      <td>如年内继续在京举办且参展中小微企业数量超过参展企业总数50%的，按照不超过实际缴纳场租费用5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f1c9eee4e5a302599f2c5baf674d05a</td>\n",
       "      <td>6488f3b3332b3995a4c60c7e3e3e9993</td>\n",
       "      <td>为切实做好新冠肺炎疫情防控工作，农业农村部启动开展“防控疫情、法治同行”专项法治宣传行动注重...</td>\n",
       "      <td>此次专项法治宣传行动注重以案释法，注重运用新媒体新技术，注重把普法融入到相关立法、执法和管理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8b85325949b73adc8a91fe7276673ac4</td>\n",
       "      <td>5e996fbd63c83563985bd24222b4a148</td>\n",
       "      <td>浙江省新型肺炎公共服务与管理平台的访问路径是什么？</td>\n",
       "      <td>路径1：打开浙里办APP首页→浙江省新型肺炎公共服务与管理平台 路径2：打开浙里办APP国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9e25cd4650d36f488811f65e6ab9a84</td>\n",
       "      <td>99c5f444f76c33589d600d7e8f487067</td>\n",
       "      <td>甘肃省平凉市政务服务中心如何应对疫情问题？</td>\n",
       "      <td>于2月3日（星期一）起，暂停对外办公，具体恢复办理时间待疫情缓解後另行通知。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                             docid  \\\n",
       "0  47a41a03966431739257ef215cdc1caa  015758c216923f89991ca61c67b29f70   \n",
       "1  17f3894afaba3f24a3ccd550fe442177  356fb8b804d73c3bab18472c36e67a3f   \n",
       "2  5f1c9eee4e5a302599f2c5baf674d05a  6488f3b3332b3995a4c60c7e3e3e9993   \n",
       "3  8b85325949b73adc8a91fe7276673ac4  5e996fbd63c83563985bd24222b4a148   \n",
       "4  c9e25cd4650d36f488811f65e6ab9a84  99c5f444f76c33589d600d7e8f487067   \n",
       "\n",
       "                                            question  \\\n",
       "0                                工业和信息化部到哪家企业进行督导检查？   \n",
       "1                         北京市对于因疫情影响暂停举办的展会项目有何支持政策？   \n",
       "2  为切实做好新冠肺炎疫情防控工作，农业农村部启动开展“防控疫情、法治同行”专项法治宣传行动注重...   \n",
       "3                          浙江省新型肺炎公共服务与管理平台的访问路径是什么？   \n",
       "4                              甘肃省平凉市政务服务中心如何应对疫情问题？   \n",
       "\n",
       "                                              answer  \n",
       "0                                       北京北铃专用汽车有限公司  \n",
       "1  如年内继续在京举办且参展中小微企业数量超过参展企业总数50%的，按照不超过实际缴纳场租费用5...  \n",
       "2  此次专项法治宣传行动注重以案释法，注重运用新媒体新技术，注重把普法融入到相关立法、执法和管理...  \n",
       "3   路径1：打开浙里办APP首页→浙江省新型肺炎公共服务与管理平台 路径2：打开浙里办APP国...  \n",
       "4             于2月3日（星期一）起，暂停对外办公，具体恢复办理时间待疫情缓解後另行通知。  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        5000 non-null   object\n",
      " 1   docid     5000 non-null   object\n",
      " 2   question  5000 non-null   object\n",
      " 3   answer    5000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edd1413c78e534afb136f36fdc9c9a00</td>\n",
       "      <td>福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230b6fc2a40937f9adf45ea97abad846</td>\n",
       "      <td>化龙桥站至李家坪站区间隧道贯通 12月30日，施工人员在重庆轨道交通九号线一期工程化龙桥站至...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8bab952dfa1a367da1b8e7ad864d766b</td>\n",
       "      <td>吉林多措并举保民生  2月18日，省政府新闻办围绕就业和医疗保障等民生问题召开新闻发布会。省...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ccc5129a465d312b9528b0b04aa4399e</td>\n",
       "      <td>山西加快推进口罩生产供应 新冠肺炎疫情发生后，我省积极督促企业扩产、转产、增产医疗防护物资。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eb04c2ada51a3161b8477de607721c64</td>\n",
       "      <td>广东：暖企添动力 扩产有信心 2月18日，位于番禺汽车城的广汽新能源，机器人不断挥舞着手臂，...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              docid  \\\n",
       "0  edd1413c78e534afb136f36fdc9c9a00   \n",
       "1  230b6fc2a40937f9adf45ea97abad846   \n",
       "2  8bab952dfa1a367da1b8e7ad864d766b   \n",
       "3  ccc5129a465d312b9528b0b04aa4399e   \n",
       "4  eb04c2ada51a3161b8477de607721c64   \n",
       "\n",
       "                                                text  \n",
       "0  福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、...  \n",
       "1  化龙桥站至李家坪站区间隧道贯通 12月30日，施工人员在重庆轨道交通九号线一期工程化龙桥站至...  \n",
       "2  吉林多措并举保民生  2月18日，省政府新闻办围绕就业和医疗保障等民生问题召开新闻发布会。省...  \n",
       "3  山西加快推进口罩生产供应 新冠肺炎疫情发生后，我省积极督促企业扩产、转产、增产医疗防护物资。...  \n",
       "4  广东：暖企添动力 扩产有信心 2月18日，位于番禺汽车城的广汽新能源，机器人不断挥舞着手臂，...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8844 entries, 0 to 8843\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   docid   8844 non-null   object\n",
      " 1   text    8844 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 138.3+ KB\n"
     ]
    }
   ],
   "source": [
    "context_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(context_df,train_df, how='left', on='docid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inds = []\n",
    "\n",
    "for i,j in zip(merge_df['text'].tolist(),merge_df['answer'].tolist()):\n",
    "    start_inds.append(i.find(j))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['answer_start'] = start_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merge_df[merge_df['answer_start'] !=-1]\n",
    "merge_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>edd1413c78e534afb136f36fdc9c9a00</td>\n",
       "      <td>福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、...</td>\n",
       "      <td>fdc51a7baeff3fafbae6736422783528</td>\n",
       "      <td>福建联合出台暖企措施支持复工稳岗的部门都有谁？</td>\n",
       "      <td>省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>edd1413c78e534afb136f36fdc9c9a00</td>\n",
       "      <td>福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、...</td>\n",
       "      <td>6b7b0c209ac939afa43f030d67433178</td>\n",
       "      <td>福建政府针对引入本地劳动力的、未经有关机构确认的疫情物资生产企业做何补助？</td>\n",
       "      <td>一次性用工服务奖补标准最高提到每人2000元。</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8bab952dfa1a367da1b8e7ad864d766b</td>\n",
       "      <td>吉林多措并举保民生  2月18日，省政府新闻办围绕就业和医疗保障等民生问题召开新闻发布会。省...</td>\n",
       "      <td>2c3b39b675493830b4d1f2f4f2a436f9</td>\n",
       "      <td>吉林省各级医疗保障部门为百姓提供哪些便捷服务？</td>\n",
       "      <td>全面实行“非必须、不窗口”经办服务，通过“网上办”“掌上办”“延长时限办”“后期补办”等方式...</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8bab952dfa1a367da1b8e7ad864d766b</td>\n",
       "      <td>吉林多措并举保民生  2月18日，省政府新闻办围绕就业和医疗保障等民生问题召开新闻发布会。省...</td>\n",
       "      <td>9a9acefa0f533ca99fd174aa89b7f600</td>\n",
       "      <td>吉林省提前预付医保基金总额达到了多少？</td>\n",
       "      <td>1.22亿元</td>\n",
       "      <td>1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>a84ad5c0fc7d3755926ec5bf2b2d9dae</td>\n",
       "      <td>重庆出台援企稳岗返还政策 2月16日，记者从市人力社保局获悉，为切实减轻中小企业负担，充分发...</td>\n",
       "      <td>a505ce9785333655aae92fabba500f70</td>\n",
       "      <td>重庆援企稳岗返还政策的申请条件是什么？</td>\n",
       "      <td>依法参加社会保险并足额缴纳2019年度社会保险费（截至2019年12月31日无欠费），企业2...</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             docid  \\\n",
       "0      0  edd1413c78e534afb136f36fdc9c9a00   \n",
       "1      1  edd1413c78e534afb136f36fdc9c9a00   \n",
       "2      3  8bab952dfa1a367da1b8e7ad864d766b   \n",
       "3      4  8bab952dfa1a367da1b8e7ad864d766b   \n",
       "4      8  a84ad5c0fc7d3755926ec5bf2b2d9dae   \n",
       "\n",
       "                                                text  \\\n",
       "0  福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、...   \n",
       "1  福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、...   \n",
       "2  吉林多措并举保民生  2月18日，省政府新闻办围绕就业和医疗保障等民生问题召开新闻发布会。省...   \n",
       "3  吉林多措并举保民生  2月18日，省政府新闻办围绕就业和医疗保障等民生问题召开新闻发布会。省...   \n",
       "4  重庆出台援企稳岗返还政策 2月16日，记者从市人力社保局获悉，为切实减轻中小企业负担，充分发...   \n",
       "\n",
       "                                 id                               question  \\\n",
       "0  fdc51a7baeff3fafbae6736422783528                福建联合出台暖企措施支持复工稳岗的部门都有谁？   \n",
       "1  6b7b0c209ac939afa43f030d67433178  福建政府针对引入本地劳动力的、未经有关机构确认的疫情物资生产企业做何补助？   \n",
       "2  2c3b39b675493830b4d1f2f4f2a436f9                吉林省各级医疗保障部门为百姓提供哪些便捷服务？   \n",
       "3  9a9acefa0f533ca99fd174aa89b7f600                    吉林省提前预付医保基金总额达到了多少？   \n",
       "4  a505ce9785333655aae92fabba500f70                    重庆援企稳岗返还政策的申请条件是什么？   \n",
       "\n",
       "                                              answer  answer_start  \n",
       "0                    省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委            41  \n",
       "1                            一次性用工服务奖补标准最高提到每人2000元。           336  \n",
       "2  全面实行“非必须、不窗口”经办服务，通过“网上办”“掌上办”“延长时限办”“后期补办”等方式...          1792  \n",
       "3                                             1.22亿元          1466  \n",
       "4  依法参加社会保险并足额缴纳2019年度社会保险费（截至2019年12月31日无欠费），企业2...           304  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4990 entries, 0 to 4989\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   index         4990 non-null   int64 \n",
      " 1   docid         4990 non-null   object\n",
      " 2   text          4990 non-null   object\n",
      " 3   id            4990 non-null   object\n",
      " 4   question      4990 non-null   object\n",
      " 5   answer        4990 non-null   object\n",
      " 6   answer_start  4990 non-null   int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 273.0+ KB\n"
     ]
    }
   ],
   "source": [
    "merge_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv('merge_data.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "#train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "#val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers,answer_inds = merge_df['text'].tolist(),merge_df['question'].tolist(),\\\n",
    "                                                             merge_df['answer'].tolist(),merge_df['answer_start'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_ends = []\n",
    "\n",
    "def add_end_idx(answers, contexts,answer_starts):\n",
    "    for answer, context ,answer_start in zip(answers, contexts,answer_starts):\n",
    "        gold_text = answer\n",
    "        start_idx = answer_start\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer_end = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer_start = start_idx - 1\n",
    "            answer_end = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer_start = start_idx - 2\n",
    "            answer_end = end_idx - 2     # When the gold label is off by two characters\n",
    "        answer_ends.append(answer_end )\n",
    "    \n",
    "    \n",
    "add_end_idx(train_answers, train_contexts,answer_inds )\n",
    "#add_end_idx(val_answers, val_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(r'chinese-bert-wwm-ext')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, \n",
    "                            train_questions,\n",
    "                            max_length = 128,\n",
    "                            truncation=True, \n",
    "                            padding=True   \n",
    "                                   )\n",
    "\n",
    "#val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委联合下发通知，出台一系列暖企措施支持疫情防控期间复工稳岗。 通知明确，切实发挥各级农民工工作领导小组办公室的统筹协调作用, 加强劳务用工有效对接，对具备外出务工条件、可成规模输送到我省用工地，并在出行前14天内及在途没有相关症状的，可由用工地和输出地联合开展“点对点、一站式”直达企业的专门运输。省级公共就业服务机构可与主要劳务输出省份签订劳务协作协议、设立劳务协作工作站，对每个工作站给予一次性10万元就业服务经费补助。鼓励优先聘用本地劳务人员。 未经省应对新冠肺炎疫情工作有关机构确认的疫情防控急需物资生产企业引进劳动力的，一次性用工服务奖补标准最高提到每人2000元。对上述企业坚持在生产一线工作的职工，给予每人每天100元的生活补助，纳入一次性用工服务奖补范畴。对春节当月至疫情一级响应结束月，采取稳定职工队伍保持连续生产的企业，给予一次性稳就业奖补。 加大失业保险稳岗返还力度，将中小微企业稳岗返还政策裁员率标准调整为不高于上年度全国调查失业率的控制目标，对参保职工30人（含）以下的企业，裁员率调整为不超过企业参保职工总数的20%。对不裁员或少裁员，符合条件的参保企业，可返还其上年度实际缴纳失业保险费的50%。对受疫情影响面临暂时性生产经营困难且恢复有望、坚持不裁员或少裁员、符合条件的参保企业，按6个月的当地月人均失业保险金和参保职工人数落实失业保险稳岗返还政策。 加强职业技能培训，鼓励技工院校学生在符合疫情防控条件下参加实习实训，探索简易岗前技能培训。对企业因生产急需新录用的人员，按每人200元标准一次性给予企业简易岗前技能培训补贴。鼓励实施线上培训，对受疫情影响的企业，在停工期、恢复期组织职工参加各类线上或线下职业培训的，可按规定纳入补贴类培训范围。 通知要求，各地要着力提升政策措施的精准度和有效性，提升各类企业享受政策措施的获得感。各类企业要落实落细防控主体责任，严格落实返岗信息登记、班车错峰接送、员工分散用餐、体温监测等具体应对措施，确保复工稳岗和疫情防控两不误。（记者 潘园园） ',\n",
       " '福建联合出台暖企措施支持复工稳岗的部门都有谁？')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[0],train_questions[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(answer_inds),len(answer_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_inds.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answer_inds[i]))\n",
    "        end_positions.append(encodings.char_to_token(i, answer_ends[i] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "    \n",
    "add_token_positions(train_encodings, train_answers)\n",
    "#add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0].overflowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "#val_dataset = SquadDataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistilBertForQuestionAnswering(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(DistilBertForQuestionAnswering, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.distilbert = DistilBertModel.from_pretrained(self.model_path)\n",
    "        self.config = self.distilbert.config\n",
    "        self.qa_outputs = torch.nn.Linear(self.config.dim, self.config.num_labels)\n",
    "        assert self.config.num_labels == 2\n",
    "        self.dropout = torch.nn.Dropout(self.config.qa_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = distilbert_output[0]  # (bs, max_query_len, dim)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)  # (bs, max_query_len, dim)\n",
    "        logits = self.qa_outputs(hidden_states)  # (bs, max_query_len, 2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)  # (bs, max_query_len)\n",
    "        end_logits = end_logits.squeeze(-1)  # (bs, max_query_len)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + distilbert_output[1:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertForQuestionAnswering\n",
    "\n",
    "#model = DistilBertForQuestionAnswering.from_pretrained(r\"G:\\2020.09.22 pytorch_pretrained_models\\bert-distil-chinese\")\n",
    "\n",
    "model = DistilBertForQuestionAnswering(r\"G:\\2020.09.22 pytorch_pretrained_models\\bert-distil-chinese\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                   | 0/624 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Overflow when unpacking long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d7552ccd727f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda20190415\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-b3ad220d0be6>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-b3ad220d0be6>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Overflow when unpacking long"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(5):\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #scheduler.step()\n",
    "       \n",
    "            \n",
    "    start_time = time.time()\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DistilBertForQuestionAnswering.from_pretrained(r\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "text = '''福建：6部门联合出台暖企措施支持复工稳岗 为解决企业复产的用工困难，经省政府同意，省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委联合下发通知，出台一系列暖企措施支持疫情防控期间复工稳岗。 通知明确，切实发挥各级农民工工作领导小组办公室的统筹协调作用, 加强劳务用工有效对接，对具备外出务工条件、可成规模输送到我省用工地，并在出行前14天内及在途没有相关症状的，可由用工地和输出地联合开展“点对点、一站式”直达企业的专门运输。省级公共就业服务机构可与主要劳务输出省份签订劳务协作协议、设立劳务协作工作站，对每个工作站给予一次性10万元就业服务经费补助。鼓励优先聘用本地劳务人员。 未经省应对新冠肺炎疫情工作有关机构确认的疫情防控急需物资生产企业引进劳动力的，一次性用工服务奖补标准最高提到每人2000元。对上述企业坚持在生产一线工作的职工，给予每人每天100元的生活补助，纳入一次性用工服务奖补范畴。对春节当月至疫情一级响应结束月，采取稳定职工队伍保持连续生产的企业，给予一次性稳就业奖补。 加大失业保险稳岗返还力度，将中小微企业稳岗返还政策裁员率标准调整为不高于上年度全国调查失业率的控制目标，对参保职工30人（含）以下的企业，裁员率调整为不超过企业参保职工总数的20%。对不裁员或少裁员，符合条件的参保企业，可返还其上年度实际缴纳失业保险费的50%。对受疫情影响面临暂时性生产经营困难且恢复有望、坚持不裁员或少裁员、符合条件的参保企业，按6个月的当地月人均失业保险金和参保职工人数落实失业保险稳岗返还政策。 加强职业技能培训，鼓励技工院校学生在符合疫情防控条件下参加实习实训，探索简易岗前技能培训。对企业因生产急需新录用的人员，按每人200元标准一次性给予企业简易岗前技能培训补贴。鼓励实施线上培训，对受疫情影响的企业，在停工期、恢复期组织职工参加各类线上或线下职业培训的，可按规定纳入补贴类培训范围。 通知要求，各地要着力提升政策措施的精准度和有效性，提升各类企业享受政策措施的获得感。各类企业要落实落细防控主体责任，严格落实返岗信息登记、班车错峰接送、员工分散用餐、体温监测等具体应对措施，确保复工稳岗和疫情防控两不误。（记者 潘园园） '''\n",
    "question = '''福建联合出台暖企措施支持复工稳岗的部门都有谁？'''\n",
    "\n",
    "inputs = tokenizer(text,question, return_tensors='pt',  truncation=True, padding=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "outputs = model( \n",
    "                input_ids = inputs['input_ids'] ,#.to(device),\n",
    "                attention_mask = inputs['attention_mask']#.to(device)\n",
    "               )\n",
    "\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ind = torch.argmax(start_scores).item()\n",
    "end_ind = torch.argmax(end_scores).item()\n",
    "print(start_ind,end_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 省人社厅、省工信厅、省教育厅、省财政厅、省交通运输厅、省卫健委\n",
    "text[start_ind :end_ind ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考材料：   \n",
    "[Question Answering with SQuAD 2.0](https://huggingface.co/transformers/custom_datasets.html#tok-ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
